{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fcc_experiment_9.x_model_superGAT_2_hyperparameters_attention_MX_SD_head_2_4_8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNOlO/vG0OxwwAc0yq+coVa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akanksha-ahuja/fcc-final-notebooks/blob/main/fcc_experiment_9_x_model_superGAT_2_hyperparameters_attention_MX_SD_head_2_4_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEvtMEMbC7JA"
      },
      "source": [
        "# Import libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b6_CQhONZUK"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "import timeit\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import distance\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IcwbaY7NkWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d34c90b-4bc4-48d7-a797-3f0e10f0e6b7"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install graphlime\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from graphlime import GraphLIME\n",
        "from torch_geometric.utils import to_networkx"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.0 MB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 222 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 376 kB 43.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.3 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting graphlime\n",
            "  Downloading graphlime-1.2.0.tar.gz (3.3 kB)\n",
            "Building wheels for collected packages: graphlime\n",
            "  Building wheel for graphlime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphlime: filename=graphlime-1.2.0-py3-none-any.whl size=2616 sha256=748145418b5225a1a406f8abcde3019f3565afcd9b2cdffb68925f1698961493\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/29/94/9835c557e2def18b58369cda0032935a3263acfa9266aaeb5d\n",
            "Successfully built graphlime\n",
            "Installing collected packages: graphlime\n",
            "Successfully installed graphlime-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHW1SAYqy9an"
      },
      "source": [
        "from torch_geometric.nn import GCNConv, TAGConv, SAGEConv, ChebConv\n",
        "from torch_geometric.nn import GATConv, SuperGATConv\n",
        "from torch_geometric.nn import GINConv\n",
        "from torch_geometric.nn import JumpingKnowledge, GCN2Conv\n",
        "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
        "from torch_geometric.nn import GNNExplainer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WR3_jqNCvyx"
      },
      "source": [
        "# Connect G drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dsc5R9Z8CyV2",
        "outputId": "4a15d255-f34d-4b60-a762-f57fe9101741"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k82Fl5gRUE3s"
      },
      "source": [
        "# Load df and process the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0erGxUrnNne9"
      },
      "source": [
        "# Data Processing Functions\n",
        "def load_df(path_to_file):\n",
        "  df = pd.read_csv(path_to_file)\n",
        "  return df\n",
        "\n",
        "def set_constants(TOTAL_EVENTS, MAX_LENGTH_EVENT=150):\n",
        "  TOTAL_EVENTS = TOTAL_EVENTS\n",
        "  MAX_LENGTH_EVENT = MAX_LENGTH_EVENT\n",
        "  return TOTAL_EVENTS, MAX_LENGTH_EVENT\n",
        "\n",
        "def create_labels(df):\n",
        "  conditions = [(df['isHiggs'] == True),(df['isZ'] == True), (df['isOther'] == True) ]\n",
        "  # create a list of the values we want to assign for each condition\n",
        "  values = [0, 1, 2] \n",
        "\n",
        "  # create a new column and use np.select to assign values to it using our lists as arguments\n",
        "  df['label'] = np.select(conditions, values)\n",
        "  return df\n",
        "\n",
        "\n",
        "def normalise_x_features(df):\n",
        "  # Normalise the features in the dataset \n",
        "  df_id = df[['event_list']]\n",
        "  df_x = df[['pid', 'pos_r', 'pos_theta', 'pos_phi', 'pos_t', 'mom_p', 'mom_theta', 'mom_phi', 'mom_mass']]\n",
        "  df_y = df[['label']]\n",
        "\n",
        "  # Create a list of labels for the new dataframe\n",
        "  new_columns = ['event_list', 'pid', 'pos_r', 'pos_theta', 'pos_phi', 'pos_t', 'mom_p', 'mom_theta', 'mom_phi', 'mom_mass', 'label']\n",
        "\n",
        "  x = df_x.values # returns numpy \n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  df_x = pd.DataFrame(x_scaled)\n",
        "\n",
        "  # Concatenate normalised x features and un-normalised y labels and event ids\n",
        "  df_normalised_features = pd.concat([df_id, df_x, df_y], axis=1)\n",
        "  df_normalised_features.columns = new_columns # You need to mention the axis\n",
        "  return df_normalised_features\n",
        "\n",
        "def split_df_by_event(df_normalised_features, TOTAL_EVENTS):\n",
        "  # Dataframes split by event \n",
        "  df_event_list = []\n",
        "  for i in range(TOTAL_EVENTS):\n",
        "    df_event = df_normalised_features[df_normalised_features['event_list']==i]\n",
        "    df_event_list.append(df_event)\n",
        "\n",
        "  # A list of number of stable particles per event \n",
        "  length_of_each_event = [len(df_event_list[i]) for i in range(len(df_event_list))]\n",
        "  return df_event_list, length_of_each_event\n",
        "\n",
        "def create_source_target_for_COO(df_event_list):\n",
        "  # Add two columns of source, target over all dataframes in df_event_list to make it compatible with pygn Data Object.\n",
        "  df_event_source_target_list = []\n",
        "  for i in range(len(df_event_list)):\n",
        "    df_event_list[i]['source'] = None\n",
        "    df_event_list[i]['target'] = None\n",
        "    df_event_source_target_list.append(df_event_list[i])\n",
        "  return df_event_source_target_list"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es0mUPY8J6NN"
      },
      "source": [
        "# Binary classification by removing the other particles "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQSmz3I8LtP4"
      },
      "source": [
        "def get_event_ids_to_be_removed(path_to_file, TOTAL_EVENTS):\n",
        "  df = load_df(path_to_file)\n",
        "  df = create_labels(df)\n",
        "  df = normalise_x_features(df)\n",
        "  df_event_list_complete = []\n",
        "  for i in range(TOTAL_EVENTS):\n",
        "      df_event = df[df['event_list']==i]\n",
        "      df_event_list_complete.append(df_event)\n",
        "\n",
        "  n_others_list = []\n",
        "  for event in df_event_list_complete:\n",
        "    labels = event['label'].to_list()\n",
        "    for node_id in range(len(event)):\n",
        "      if labels[node_id] == 2:\n",
        "        n_others_list.append(event['event_list'].values)\n",
        "\n",
        "  event_ids_others = np.asarray(n_others_list)\n",
        "  event_ids_others_unique = []\n",
        "  for event_ids in event_ids_others:\n",
        "    event_ids_others_unique.append(np.unique(event_ids))\n",
        "\n",
        "  event_ids_to_be_removed = np.unique(np.asarray(event_ids_others_unique))\n",
        "  return df, event_ids_to_be_removed\n",
        "\n",
        "\n",
        "def get_updated_TOTAL_EVENTS(event_ids_to_be_removed, TOTAL_EVENTS):\n",
        "  NO_OF_EVENTS_REMOVED = len(event_ids_to_be_removed)\n",
        "  TOTAL_EVENTS -= NO_OF_EVENTS_REMOVED\n",
        "  return TOTAL_EVENTS\n",
        "\n",
        "def drop_all_rows_with_event_ids(df, event_ids_to_be_removed):\n",
        "  for event_id in event_ids_to_be_removed:\n",
        "    # print(event_id)\n",
        "    df.drop(df[df['event_list']== event_id].index, inplace=True)\n",
        "  # Save the file for manual fix \n",
        "  df['event_list'].to_csv('fix_column.csv')\n",
        "  return \"Saved\"\n",
        "\n",
        "# The fixed column is a file for 10,000 events, and you are processing all those events \n",
        "# If you want to process lesser events, update this method to choose a specific number of event ids\n",
        "def get_df_with_fixed_event_id_column(df, path_to_file):\n",
        "  df_new_column = pd.read_csv(\"/content/fixed_column.csv\")\n",
        "  new_column = df_new_column['Unnamed: 2'].values\n",
        "  # Drop ['event_list] \n",
        "  df = df.drop(\"event_list\", axis=1)\n",
        "  # df = df.drop(\"Unnamed: 0\", axis=1)\n",
        "  # Replace the existing column with a new column \n",
        "  df['event_list'] = new_column\n",
        "  column_names = [\"event_list\",\"pid\",\t\"pos_r\",\t\"pos_theta\",\t\"pos_phi\",\t\"pos_t\",\t\"mom_p\",\t\"mom_theta\",\t\"mom_phi\"\t,\"mom_mass\"\t,\"label\"]\n",
        "  df = df.reindex(columns=column_names)\n",
        "  return df\n",
        "\n",
        "def create_df_with_only_H_Z_events(path_to_file=\"/content/output_11_07_2021.csv\", path_to_file_fixed_columns='/content/fixed_column.csv', TOTAL_EVENTS=10000):\n",
        "  df, event_ids_to_be_removed = get_event_ids_to_be_removed(path_to_file, TOTAL_EVENTS)\n",
        "  TOTAL_EVENTS = get_updated_TOTAL_EVENTS(event_ids_to_be_removed, TOTAL_EVENTS)\n",
        "  drop_all_rows_with_event_ids(df, event_ids_to_be_removed)\n",
        "  df = get_df_with_fixed_event_id_column(df, path_to_file_fixed_columns)\n",
        "  return df, TOTAL_EVENTS \n",
        "  \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeZ9JUy7ts2t"
      },
      "source": [
        "#Fixed Graph Sizes need further processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icIeWl-XRxLv"
      },
      "source": [
        "# Fixed Graph Sizes need further processing \n",
        "def process_df_create_fixed_size_events(df_single_event, MAX_LENGTH_EVENT, append_with_digits= 9999.0, category_assigned=3, label_column=-3):\n",
        "  df_processed = df_single_event\n",
        "  df_processed = df_processed.drop('event_list', axis=1)\n",
        "\n",
        "  # Append the rows\n",
        "  n_rows_to_append = MAX_LENGTH_EVENT - len(df_processed)\n",
        "  # Creating the appending df \n",
        "  np_append = np.ones((n_rows_to_append, len(df_processed.columns)))\n",
        "  # print(np_append)\n",
        "  np_append = np_append * append_with_digits\n",
        "  for x in range(n_rows_to_append):\n",
        "    np_append[x][label_column] = category_assigned # Label Column\n",
        "    np_append[x][-2] = None # Source Column \n",
        "    np_append[x][-1] = None # Target Column \n",
        "\n",
        "  # print(np_append)\n",
        "  df_append = pd.DataFrame(data=np_append, columns=df_processed.columns)\n",
        "  # Concat the two dfs\n",
        "  df_fixed_size_event = pd.concat([df_processed, df_append])\n",
        "  # Reset the index \n",
        "  df_fixed_size_event.reset_index(drop=True) # Drop the old index colum\n",
        "  return df_fixed_size_event\n",
        "\n",
        "def process_all_events(df_event_list, TOTAL_EVENTS, MAX_LENGTH_EVENT):\n",
        "  df_event_processed_list = []\n",
        "  for i in range(TOTAL_EVENTS):\n",
        "    df_processed = process_df_create_fixed_size_events(df_event_list[i], MAX_LENGTH_EVENT)\n",
        "    df_event_processed_list.append(df_processed)\n",
        "  return df_event_processed_list\n",
        "\n",
        "def clean_columns(df_event_processed_list, TOTAL_EVENTS):\n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    df_event_processed_list[event_id]['source'] = None\n",
        "    df_event_processed_list[event_id]['target'] = None\n",
        "  return df_event_processed_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGWiu1jwaDGU"
      },
      "source": [
        "#Generate Data.x and Data.y for pytorch geometric "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKRkJ576tz4R"
      },
      "source": [
        "def generate_graph_data_x_y_for_already_normalised_features(df_event_processed_list_cleaned, TOTAL_EVENTS):\n",
        "  # Generating data.x and data.y for pytorch geomteric \n",
        "  graph_data_x_list = []\n",
        "  graph_data_y_list = []\n",
        "  for i in range(TOTAL_EVENTS):\n",
        "    df_graph = df_event_processed_list_cleaned[i]\n",
        "    # Extract node features and labels from cleaned processed fixed size event list and convert to numpy \n",
        "    data_x = df_graph[['pid', 'pos_r', 'pos_theta', 'pos_phi', 'pos_t', 'mom_p', 'mom_theta', 'mom_phi', 'mom_mass']].to_numpy()\n",
        "    data_y = df_graph[['label']].to_numpy()\n",
        "\n",
        "    # Convert numpy objects into tensors for data loaders \n",
        "    graph_data_x_list.append(torch.Tensor(data_x))\n",
        "    graph_data_y_list.append(torch.Tensor(data_y))\n",
        "  return graph_data_x_list, graph_data_y_list"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDq9YGw0YfRw"
      },
      "source": [
        "# NORMALISES THE APPENDED PADDED NUMBERS - for FIXED SIZE GRAPHS \n",
        "def generate_graph_data_x_y(df_event_processed_list_cleaned, TOTAL_EVENTS):\n",
        "  # Generating data.x and data.y for pytorch geomteric \n",
        "  graph_data_x_list = []\n",
        "  graph_data_y_list = []\n",
        "  for i in range(TOTAL_EVENTS):\n",
        "    df_graph = df_event_processed_list_cleaned[i]\n",
        "    df_x = df_graph[['pid', 'pos_r', 'pos_theta', 'pos_phi', 'pos_t', 'mom_p', 'mom_theta', 'mom_phi', 'mom_mass']]\n",
        "    df_y = df_graph[['label']]\n",
        "\n",
        "    # Normalise the features in the dataset \n",
        "    x = df_x.values # returns numpy \n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    x_scaled = min_max_scaler.fit_transform(x)\n",
        "    df_x = pd.DataFrame(x_scaled)\n",
        "\n",
        "    # Convert the pandas dataframe to numpy \n",
        "    x_data = df_x.to_numpy()\n",
        "    y_data = df_y.to_numpy()\n",
        "\n",
        "    # Convert numpy objects into tensors for data loaders \n",
        "    x_data_tensor = torch.Tensor(x_data)\n",
        "    y_data_tensor = torch.Tensor(y_data)\n",
        "\n",
        "    # what is the difference btw FloatTensor and Tensor? \n",
        "    graph_data_x_list.append(x_data_tensor)\n",
        "    graph_data_y_list.append(y_data_tensor)\n",
        "\n",
        "  return graph_data_x_list, graph_data_y_list"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4bZZg6oD_Ua"
      },
      "source": [
        "# Create graph nodes and labels "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKW5T1fXt3aB"
      },
      "source": [
        "def create_graph_nodes_and_labels_for_variable_graphs(path_to_file, TOTAL_EVENTS):\n",
        "  df = load_df(path_to_file) # You can specify path to file here \n",
        "  TOTAL_EVENTS, _ = set_constants(TOTAL_EVENTS) # you can pass the constants here \n",
        "  df = create_labels(df) \n",
        "  df_normalised_features = normalise_x_features(df) # Don't call this if you are normalising features when creating graph dataset for FIXED GRAPHS  \n",
        "  df_event_list, length_of_each_event = split_df_by_event(df_normalised_features, TOTAL_EVENTS)\n",
        "  df_event_source_target_list = create_source_target_for_COO(df_event_list) \n",
        "  df_event_processed_list_cleaned = df_event_source_target_list\n",
        "  graph_data_x_list, graph_data_y_list = generate_graph_data_x_y_for_already_normalised_features(df_event_processed_list_cleaned, TOTAL_EVENTS)\n",
        "  return df, df_normalised_features, df_event_list, length_of_each_event, df_event_processed_list_cleaned, graph_data_x_list, graph_data_y_list"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7M83YLVbNRY"
      },
      "source": [
        "def create_graph_nodes_and_labels_for_fixed_graphs(path_to_file, TOTAL_EVENTS, MAX_LENGTH_EVENT):\n",
        "  df = load_df(path_to_file) # You can specify path to file here \n",
        "  TOTAL_EVENTS, MAX_LENGTH_EVENT = set_constants(TOTAL_EVENTS, MAX_LENGTH_EVENT) # you can pass the constants here \n",
        "  df = create_labels(df) \n",
        "  # df_normalised_features = normalise_x_features(df) # Don't call this if you are normalising features when creating graph dataset for FIXED GRAPHS  \n",
        "  df_normalised_features = df\n",
        "  df_event_list, length_of_each_event = split_df_by_event(df_normalised_features, TOTAL_EVENTS)\n",
        "  df_event_source_target_list = create_source_target_for_COO(df_event_list) \n",
        "  df_event_processed_list = process_all_events(df_event_list, TOTAL_EVENTS, MAX_LENGTH_EVENT)\n",
        "  df_event_processed_list_cleaned = clean_columns(df_event_processed_list, TOTAL_EVENTS)\n",
        "  graph_data_x_list, graph_data_y_list = generate_graph_data_x_y(df_event_processed_list_cleaned, TOTAL_EVENTS)\n",
        "  return df, df_normalised_features, df_event_list, length_of_each_event, df_event_processed_list_cleaned, graph_data_x_list, graph_data_y_list"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLSE5IJkxRVH"
      },
      "source": [
        "# create_df_with_only_H_Z_events\n",
        "def create_graph_nodes_and_labels_for_variable_graphs_H_Z(path_to_file, path_to_file_fixed_columns, TOTAL_EVENTS):\n",
        "  df, TOTAL_EVENTS = create_df_with_only_H_Z_events(path_to_file, path_to_file_fixed_columns, TOTAL_EVENTS)\n",
        "  df_event_list, length_of_each_event = split_df_by_event(df, TOTAL_EVENTS)\n",
        "  df_event_source_target_list = create_source_target_for_COO(df_event_list) \n",
        "  df_event_processed_list_cleaned = df_event_source_target_list\n",
        "  graph_data_x_list, graph_data_y_list = generate_graph_data_x_y_for_already_normalised_features(df_event_processed_list_cleaned, TOTAL_EVENTS)\n",
        "  return df, df_event_list, length_of_each_event, df_event_processed_list_cleaned, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70EA4OHKS5lg"
      },
      "source": [
        "# Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXdzHQ_aSBlp"
      },
      "source": [
        "def generate_particle_lists_for_label_connections(df_event_list):\n",
        "  # Getting all the lists for each event in the dataset - define h_list, z_list, o_list\n",
        "  h_list = [] # all higgs nodes are connected Ω\n",
        "  z_list = []  # all z nodes are connected \n",
        "  o_list = [] # all otehr nodes are connected \n",
        "\n",
        "  for i in range(len(df_event_list)):\n",
        "    df_event = df_event_list[i]\n",
        "    h = df_event[df_event['label']==0]\n",
        "    h.reset_index(drop=True)\n",
        "    z = df_event[df_event['label']==1]\n",
        "    z.reset_index(drop=True)\n",
        "    o = df_event[df_event['label']==2] \n",
        "    o.reset_index(drop=True)\n",
        "    h_list.append(h)\n",
        "    z_list.append(z)\n",
        "    o_list.append(o)\n",
        "\n",
        "  return h_list, z_list, o_list\n",
        "\n",
        "def generate_X_label_list(h_list, z_list, o_list, df_event_list, TOTAL_EVENTS):\n",
        "  Z_BOSON = int(1)\n",
        "  H_BOSON = int(0)\n",
        "  NO_BOSON = int(2)\n",
        "  # NO_PARTICLE = int(3)\n",
        "  X_list = []\n",
        "  # for each event event_id \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    X = df_event_list[event_id]\n",
        "    source_list, target_list  = [], []\n",
        "    # for each node_id \n",
        "    for node_id in range(len(X)):\n",
        "      if X.iloc[node_id].label == Z_BOSON:\n",
        "        source = [node_id for x in range(len(z_list[event_id]))]\n",
        "        target = [x for x in range(len(z_list[event_id]))]\n",
        "        source_list.append(source)\n",
        "        target_list.append(target)\n",
        "      elif X.iloc[node_id].label == H_BOSON:\n",
        "        source = [node_id for x in range(len(h_list[event_id]))]\n",
        "        target = [x for x in range(len(h_list[event_id]))]\n",
        "        source_list.append(source)\n",
        "        target_list.append(target)\n",
        "      elif X.iloc[node_id].label == NO_BOSON:\n",
        "        source = [node_id for x in range(len(o_list[event_id]))]\n",
        "        target = [x for x in range(len(o_list[event_id]))]\n",
        "        source_list.append(source)\n",
        "        target_list.append(target)\n",
        "      # Add all values as 2 columns for eache event   \n",
        "    X['source'] = source_list\n",
        "    X['target'] = target_list\n",
        "    X_list.append(X)\n",
        "\n",
        "  return X_list\n",
        "\n",
        "def convert_coo_format_for_label_events(df):\n",
        "    source_list = list(itertools.chain.from_iterable(df['source'].to_numpy())) \n",
        "    target_list = list(itertools.chain.from_iterable(df['target'].to_numpy()))\n",
        "    edge_index= torch.tensor([source_list, target_list], dtype=torch.long)\n",
        "    return edge_index\n",
        "    \n",
        "def create_COO_format_data_label_list(X_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS):\n",
        "  # Data Represented as edges with same labels \n",
        "  # A list of graph data items to be passed on to the data loader \n",
        "  data_label_list = []\n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    data_item = Data(x = graph_data_x_list[event_id], \n",
        "                     y = graph_data_y_list[event_id], \n",
        "                    edge_index = convert_coo_format_for_label_events(X_list[event_id]))\n",
        "    data_label_list.append(data_item)\n",
        "  return data_label_list"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVxBwcDVbG1v"
      },
      "source": [
        "def create_data_label_list(df_event_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS):\n",
        "  # Create Data Label List \n",
        "  h_list, z_list, o_list  = generate_particle_lists_for_label_connections(df_event_list)\n",
        "  X_list = generate_X_label_list(h_list, z_list, o_list, df_event_list, TOTAL_EVENTS)\n",
        "  data_label_list = create_COO_format_data_label_list(X_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS)\n",
        "  return data_label_list "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNfBulnuTsWR"
      },
      "source": [
        "# Radius\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7T3BcK7TByA"
      },
      "source": [
        "def get_features_extraction_list(df_event_list, TOTAL_EVENTS):\n",
        "  feature_extraction_list = [] \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    event_features = df_event_list[event_id][[\"pid\",\t\"pos_r\",\t\"pos_theta\",\t\"pos_phi\",\t\"pos_t\"\t,\"mom_p\",\t\"mom_theta\",\t\"mom_phi\",\t\"mom_mass\"]]\n",
        "    feature_extraction_list.append(event_features)\n",
        "  return feature_extraction_list\n",
        "\n",
        "def get_PCA_transformed_features(feature_extraction_list, TOTAL_EVENTS):\n",
        "  X_pca_list = []   \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    pca = PCA() \n",
        "    X_pca = pca.fit_transform(feature_extraction_list[event_id])\n",
        "    X_pca_list.append(X_pca)\n",
        "  return X_pca_list\n",
        "\n",
        "def get_2_D_coordinates(X_pca_list, length_of_each_event, TOTAL_EVENTS):\n",
        "  # Find all 2-d coordinates \n",
        "  point_event_list = [] # stores all points for each event in a list of tuple points\n",
        "  index_event_list = [] # stores all indices for each event in a list of tuple indices\n",
        "  principal_components_list = [5, 6] # after data exploration, these two were chosen \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    length = length_of_each_event[event_id]\n",
        "    points = []\n",
        "    index_list = []\n",
        "    # print(length)\n",
        "    for node_id_source in range(length):\n",
        "      for node_id_target in range(length):\n",
        "        pt = (X_pca_list[event_id][node_id_source, principal_components_list[0]],\n",
        "              X_pca_list[event_id][node_id_target, principal_components_list[1]])\n",
        "        index_list.append((node_id_source, node_id_target))\n",
        "        points.append(pt)\n",
        "    point_event_list.append(points)\n",
        "    index_event_list.append(index_list)\n",
        "  return point_event_list, index_event_list\n",
        "\n",
        "def calculate_euclidean_distance(point_event_list, length_of_each_event, TOTAL_EVENTS):\n",
        "  # Calculate euclidean distance between each consecutive pair \n",
        "  distance_event_list = []\n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    event_length = length_of_each_event[event_id]\n",
        "    distance_list = []\n",
        "    for node_id_source in range(event_length):\n",
        "      for node_id_target in range(event_length):\n",
        "        # print(k, length_of_each_event[k], i, j)\n",
        "        xpt = point_event_list[event_id][node_id_source][0]\n",
        "        ypt = point_event_list[event_id][node_id_target][1]\n",
        "        dist = distance.euclidean(xpt,ypt)\n",
        "        distance_list.append(dist)\n",
        "    distance_event_list.append(distance_list)\n",
        "  return distance_event_list\n",
        "\n",
        "def calculate_node_distances_by_event(distance_event_list, length_of_each_event, TOTAL_EVENTS):\n",
        "  # Calculating the each node's distances for event_length for each event for 10,000 events\n",
        "  distance_each_particle_event_list = []\n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    distance_each_particle_list = np.array_split(distance_event_list[event_id], length_of_each_event[event_id]) \n",
        "    distance_each_particle_event_list.append(distance_each_particle_list)\n",
        "  return distance_each_particle_event_list\n",
        "\n",
        "def calculate_edges_by_radius(distance_each_particle_event_list, length_of_each_event, TOTAL_EVENTS, radius = 0.2):\n",
        "  # Fixing radius at random or threshold to be 0.2, therefore all node_ids with distances less than this threshold, is added to the neighbour_list \n",
        "  neighbour_event_list = []\n",
        "  target_event_list = []\n",
        "  source_event_list = [] \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    event_length = length_of_each_event[event_id]\n",
        "    neighbour_list = []\n",
        "    source_list = []\n",
        "    target_list = []\n",
        "    for node_id_source in range(event_length):\n",
        "      for node_id_target in range(event_length):\n",
        "        if distance_each_particle_event_list[event_id][node_id_source][node_id_target] <= radius:\n",
        "            source_list.append(node_id_source)\n",
        "            target_list.append(node_id_target)\n",
        "            neighbour_list.append((node_id_source, node_id_target))\n",
        "    neighbour_event_list.append(neighbour_list)\n",
        "    target_event_list.append(target_list)\n",
        "    source_event_list.append(source_list)\n",
        "  return neighbour_event_list, target_event_list, source_event_list\n",
        "\n",
        "def convert_coo_format_for_radius_events(source_event_list,target_event_list, event_id):\n",
        "    edge_index= torch.tensor([source_event_list[event_id], target_event_list[event_id]], dtype=torch.long)\n",
        "    return edge_index\n",
        "\n",
        "def create_COO_format_data_radius_list(source_event_list,target_event_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS):\n",
        "  # Data Represented as edges within Radius = 0.2 \n",
        "  # A list of graph data items to be passed on to the data loader \n",
        "  data_radius_list = []\n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    data_item = Data(x = graph_data_x_list[event_id], \n",
        "                    y = graph_data_y_list[event_id], \n",
        "                    edge_index = convert_coo_format_for_radius_events(source_event_list,target_event_list, event_id))\n",
        "    data_radius_list.append(data_item)\n",
        "  return data_radius_list"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZREpIC_cg-0C"
      },
      "source": [
        "def create_data_radius_list(df_event_list, length_of_each_event, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, radius = 0.2):\n",
        "  # Create Data Radius List \n",
        "  feature_extraction_list = get_features_extraction_list(df_event_list, TOTAL_EVENTS)\n",
        "  X_pca_list = get_PCA_transformed_features(feature_extraction_list, TOTAL_EVENTS) \n",
        "  point_event_list, index_event_list = get_2_D_coordinates(X_pca_list, length_of_each_event, TOTAL_EVENTS) \n",
        "  distance_event_list = calculate_euclidean_distance(point_event_list, length_of_each_event, TOTAL_EVENTS)\n",
        "  distance_each_particle_event_list = calculate_node_distances_by_event(distance_event_list, length_of_each_event,  TOTAL_EVENTS)\n",
        "  neighbour_event_list, target_event_list, source_event_list = calculate_edges_by_radius(distance_each_particle_event_list, length_of_each_event, TOTAL_EVENTS, radius = 0.2)\n",
        "  data_radius_list = create_COO_format_data_radius_list(source_event_list,target_event_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS)\n",
        "  return data_radius_list "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHqYChKxURgj"
      },
      "source": [
        "# KNN \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iiupWWVUQ9W"
      },
      "source": [
        "def define_knn(num_neighbours=8):\n",
        "  knn = NearestNeighbors(n_neighbors=num_neighbours, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\n",
        "  return knn\n",
        "\n",
        "def generate_X_list_knn(knn, df_event_list, TOTAL_EVENTS, num_neighbours=8):\n",
        "  X_list_knn = [] \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    X = df_event_list[event_id]\n",
        "    X = X.drop(columns=['source', 'target'])\n",
        "    knn.fit(X)\n",
        "    neighbour = knn.kneighbors(X, n_neighbors=num_neighbours, return_distance=False)\n",
        "    target = neighbour\n",
        "    source = np.zeros((neighbour.shape))\n",
        "    X['source'] = None\n",
        "    X['target'] = None\n",
        "    for i in range(len(X)):\n",
        "      X['source'].iloc[i] = np.ones((neighbour.shape)) * i\n",
        "      X['target'].iloc[i] = target[i]\n",
        "    X_list_knn.append(X)\n",
        "  return X_list_knn\n",
        "\n",
        "def convert_COO_for_knn_events(df, num_neighbours=8):\n",
        "  source_list = [] \n",
        "  for i in range(len(df)):\n",
        "    for _ in range(num_neighbours):\n",
        "      source_list.append(i)\n",
        "  target_list = list(itertools.chain.from_iterable(df['target'].to_numpy()))\n",
        "  edge_index= torch.tensor([source_list, target_list], dtype=torch.long)\n",
        "  return edge_index\n",
        "\n",
        "def create_COO_format_data_knn_list(X_list_knn, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, num_neighbours=8):\n",
        "  # Data Represented as edges with K-nearest neighbours as 8 \n",
        "  # A list of graph data items to be passed on to the data loader \n",
        "  data_knn_list = []\n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    data_item = Data(x = graph_data_x_list[event_id], \n",
        "                    y = graph_data_y_list[event_id], \n",
        "                    edge_index = convert_COO_for_knn_events(X_list_knn[event_id], num_neighbours=8))\n",
        "    data_knn_list.append(data_item)\n",
        "  return data_knn_list"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsEQXxVriEez"
      },
      "source": [
        "def create_data_knn_list(df_event_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, num_neighbours=8):\n",
        "  # Create data knn list \n",
        "  knn = define_knn(num_neighbours=8)\n",
        "  X_list_knn = generate_X_list_knn(knn, df_event_list, TOTAL_EVENTS, num_neighbours=8)\n",
        "  data_knn_list = create_COO_format_data_knn_list(X_list_knn, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, num_neighbours=8)\n",
        "  return data_knn_list "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D21onOFjmFR"
      },
      "source": [
        "# Create all 3 data sets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG5Ol4VZivxN"
      },
      "source": [
        "def get_data_lists_for_fixed_size_graphs(path_to_file, TOTAL_EVENTS, MAX_LENGTH_EVENT):\n",
        "  # FIXED SIZE GRAPHS\n",
        "  df, df_normalised_features, df_event_list, length_of_each_event, df_event_processed_list_cleaned, graph_data_x_list, graph_data_y_list = create_graph_nodes_and_labels_for_fixed_graphs(path_to_file, TOTAL_EVENTS, MAX_LENGTH_EVENT)\n",
        "  data_label_list = create_data_label_list(df_event_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS)\n",
        "  data_radius_list = create_data_radius_list(df_event_list, length_of_each_event, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, radius = 0.2)\n",
        "  data_knn_list  = create_data_knn_list(df_event_list,graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, num_neighbours=8)\n",
        "  return data_radius_list, data_knn_list, data_label_list, df_event_list, length_of_each_event, df_event_processed_list_cleaned"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e7AAkjRk4Jr"
      },
      "source": [
        "Returning df_event_list, length_of_each_event, df_event_processed_list_cleaned for calculating mass of particles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9BTgr6suKo8"
      },
      "source": [
        "def get_data_lists_for_variable_size_graphs(path_to_file, TOTAL_EVENTS):\n",
        "  # VARIABLE SIZE GRAPHS\n",
        "  df, df_normalised_features, df_event_list, length_of_each_event, df_event_processed_list_cleaned, graph_data_x_list, graph_data_y_list = create_graph_nodes_and_labels_for_variable_graphs(path_to_file, TOTAL_EVENTS)\n",
        "  data_label_list = create_data_label_list(df_event_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS)\n",
        "  data_radius_list = create_data_radius_list(df_event_list, length_of_each_event, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, radius = 0.2)\n",
        "  data_knn_list  = create_data_knn_list(df_event_list,graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, num_neighbours=8)\n",
        "  return data_radius_list, data_knn_list, data_label_list, df_event_list, length_of_each_event, df_event_processed_list_cleaned"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzP6Gsx9zCCO"
      },
      "source": [
        "Create data lists for variable size graphs for H and Z particles only "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdCW-OUT9mqA"
      },
      "source": [
        "def get_data_lists_for_variable_size_graphs_H_Z(path_to_file=\"/content/output_11_07_2021.csv\", path_to_file_fixed_columns='/content/fixed_column.csv', TOTAL_EVENTS=10000):\n",
        "  df, df_event_list, length_of_each_event, df_event_processed_list_cleaned, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS = create_graph_nodes_and_labels_for_variable_graphs_H_Z(path_to_file, path_to_file_fixed_columns, TOTAL_EVENTS)\n",
        "  data_label_list = create_data_label_list(df_event_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS)\n",
        "  data_radius_list = create_data_radius_list(df_event_list, length_of_each_event, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, radius = 0.2)\n",
        "  data_knn_list  = create_data_knn_list(df_event_list,graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, num_neighbours=8)\n",
        "  return data_radius_list, data_knn_list, data_label_list, df_event_list, length_of_each_event, df_event_processed_list_cleaned, TOTAL_EVENTS\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu5OH0MYRn6g"
      },
      "source": [
        "# calculate true mass and predicted mass for variable graphs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzaxkTiXGRTd"
      },
      "source": [
        "def get_true_mass_list(data_list, df_event_processed_list_cleaned, TOTAL_EVENTS):\n",
        "  total_true_mass_H_list = []\n",
        "  total_true_mass_Z_list = []\n",
        "  total_true_mass_O_list = []\n",
        "  column_index_for_mass = -1 # fixed \n",
        "  column_index_for_label = 0 # fixed for true labels\n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    true_mass_H_list = []\n",
        "    true_mass_Z_list = []\n",
        "    true_mass_O_list = []\n",
        "    event_length = len(df_event_processed_list_cleaned[event_id])\n",
        "    for node_id in range(event_length):\n",
        "      if data_list[event_id].y[node_id].numpy()[column_index_for_label] == 0:\n",
        "        true_mass_H_list.append(data_list[event_id].x[node_id].numpy()[column_index_for_mass])\n",
        "      elif data_list[event_id].y[node_id].numpy()[column_index_for_label] == 1:\n",
        "        true_mass_Z_list.append(data_list[event_id].x[node_id].numpy()[column_index_for_mass])\n",
        "      elif data_list[event_id].y[node_id].numpy()[column_index_for_label] == 2:\n",
        "        true_mass_O_list.append(data_list[event_id].x[node_id].numpy()[column_index_for_mass])\n",
        "    total_true_mass_H_list.append(true_mass_H_list)\n",
        "    total_true_mass_Z_list.append(true_mass_Z_list)\n",
        "    total_true_mass_O_list.append(true_mass_O_list)\n",
        "  return total_true_mass_H_list, total_true_mass_Z_list, total_true_mass_O_list"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6mUc4ngmqdv"
      },
      "source": [
        "def calculate_predicted_mass(data_list, df_event_processed_list_cleaned, pred_list, TOTAL_EVENTS):\n",
        "  total_pred_mass_H_list = []\n",
        "  total_pred_mass_Z_list = []\n",
        "  total_pred_mass_O_list = []\n",
        "  column_index_for_mass = -1 # fixed \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    pred_mass_H_list = []\n",
        "    pred_mass_Z_list = []\n",
        "    pred_mass_O_list = []\n",
        "    event_length = len(df_event_processed_list_cleaned[event_id])\n",
        "    for node_id in range(event_length):\n",
        "      pred = pred_list[event_id][node_id].numpy()\n",
        "      if pred == 0:\n",
        "        pred_mass_H_list.append(data_list[event_id].x[node_id].numpy()[column_index_for_mass])\n",
        "      elif pred == 1:\n",
        "        pred_mass_Z_list.append(data_list[event_id].x[node_id].numpy()[column_index_for_mass])\n",
        "      elif pred == 2:\n",
        "        pred_mass_O_list.append(data_list[event_id].x[node_id].numpy()[column_index_for_mass])\n",
        "    total_pred_mass_H_list.append(pred_mass_H_list)\n",
        "    total_pred_mass_Z_list.append(pred_mass_Z_list)\n",
        "    total_pred_mass_O_list.append(pred_mass_O_list)\n",
        "  return total_pred_mass_H_list, total_pred_mass_Z_list, total_pred_mass_O_list"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzzmg0KES7aq"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCzaa6_3SMLF"
      },
      "source": [
        "# For Fixed Graph Size\n",
        "def set_model_constants():\n",
        "  dataset_num_features = 9 \n",
        "  dataset_num_classes = 4\n",
        "  return dataset_num_features, dataset_num_classes"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpJNEBh5Z5rb"
      },
      "source": [
        "# For Variable Graph Size \n",
        "def set_model_constants_variable():\n",
        "  dataset_num_features = 9 \n",
        "  dataset_num_classes = 3\n",
        "  return dataset_num_features, dataset_num_classes"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We-gIE-aAsxP"
      },
      "source": [
        "# For Variable Graph Size - H and Z events\n",
        "def set_model_constants_variable_H_Z():\n",
        "  dataset_num_features = 9 \n",
        "  dataset_num_classes = 2\n",
        "  return dataset_num_features, dataset_num_classes"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHNPHdAKSN0b"
      },
      "source": [
        "class SuperGAT(torch.nn.Module):\n",
        "    def __init__(self, dataset_num_features,hidden_channels,dataset_num_classes, in_heads=8, out_heads=1, dropout=0.5, attention_type='MX', edge_sample_ratio=0.8):\n",
        "        super(SuperGAT, self).__init__()\n",
        "        self.conv1 = SuperGATConv(dataset_num_features, hidden_channels, heads=in_heads, dropout=dropout, attention_type=attention_type, edge_sample_ratio=edge_sample_ratio, is_undirected=True)\n",
        "        self.conv2 = SuperGATConv(hidden_channels * in_heads, dataset_num_classes, heads=out_heads, concat=False, dropout=dropout,attention_type=attention_type, edge_sample_ratio=edge_sample_ratio, is_undirected=True)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        att_loss = self.conv1.get_attention_loss()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        att_loss += self.conv2.get_attention_loss()\n",
        "        return F.log_softmax(x, dim=-1), att_loss\n",
        "\n",
        "\n",
        "# Define the model \n",
        "class SuperGAT2(torch.nn.Module):\n",
        "    def __init__(self, dataset_num_features, hidden_channels, dataset_num_classes):\n",
        "        super(SuperGAT2, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = SuperGATConv(dataset_num_features, hidden_channels)\n",
        "        self.conv2 = SuperGATConv(hidden_channels, dataset_num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class SuperGAT4(torch.nn.Module):\n",
        "    def __init__(self, dataset_num_features, hidden_channels, dataset_num_classes):\n",
        "        super(SuperGAT4, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = SuperGATConv(dataset_num_features, hidden_channels)\n",
        "        self.conv2 = SuperGATConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = SuperGATConv(hidden_channels, hidden_channels)\n",
        "        self.conv4 = SuperGATConv(hidden_channels, dataset_num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv4(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3o_Eum1yPVO"
      },
      "source": [
        "# Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBErN1WaSQL0"
      },
      "source": [
        "def create_data_loaders(data_list, train_split=0.6):\n",
        "  # get length of the list\n",
        "  lenth_of_list = len(data_list)\n",
        "  train_split = 0.6\n",
        "  valid_split = 0.2\n",
        "  test_split = 0.2\n",
        "\n",
        "  # create indices for loaders \n",
        "  train_end_index = int(lenth_of_list * train_split)\n",
        "  validate_start_index = train_end_index\n",
        "  validate_end_index = validate_start_index + int(lenth_of_list * valid_split)\n",
        "  test_start_index = validate_end_index\n",
        "\n",
        "  train_loader = DataLoader(data_list[:train_end_index], batch_size=1, shuffle=False)\n",
        "  validation_loader = DataLoader(data_list[validate_start_index:validate_end_index], batch_size=1, shuffle=False)\n",
        "  test_loader = DataLoader(data_list[test_start_index:], batch_size=1, shuffle=False)\n",
        "  \n",
        "  return train_loader, validation_loader, test_loader, train_end_index, validate_start_index, validate_end_index, test_start_index "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Y8qbt-yRRt"
      },
      "source": [
        "# Train, Validate, Test, Cal Accuracy and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpJftQCMK62Q"
      },
      "source": [
        "# SuperGAT requires it's own set of training functions due to attention loss \n",
        "def train(model, optimizer, loader):\n",
        "  model.train()\n",
        "  pred_list, y_list, loss_list, y_list_numpy, pred_list_numpy = [], [],[], [], []\n",
        "  for batch in loader:\n",
        "    x, edges = batch.x, batch.edge_index\n",
        "    y = batch.y\n",
        "    y_list.append(y.squeeze(1).type(torch.LongTensor))\n",
        "    y_list_numpy.append(y.squeeze(1).type(torch.LongTensor).numpy())\n",
        "    optimizer.zero_grad()\n",
        "    out, att_loss = model(x, edges)\n",
        "    loss = criterion(out, y.squeeze(1).type(torch.LongTensor))\n",
        "    loss += 4.0 * att_loss\n",
        "    loss_list.append(loss)\n",
        "    pred = out.argmax(dim=1)\n",
        "    pred_list.append(pred) \n",
        "    pred_list_numpy.append(pred.numpy()) \n",
        "    loss.backward()\n",
        "    optimizer.step()  \n",
        "  \n",
        "  return loss_list, pred_list, y_list, y_list_numpy, pred_list_numpy\n",
        "\n",
        "def validate(model, optimizer, loader):\n",
        "  model.train()\n",
        "  pred_list, y_list, loss_list, y_list_numpy, pred_list_numpy = [], [],[], [], []\n",
        "  for batch in loader:\n",
        "    x, edges = batch.x, batch.edge_index\n",
        "    y = batch.y\n",
        "    y_list.append(y.squeeze(1).type(torch.LongTensor))\n",
        "    y_list_numpy.append(y.squeeze(1).type(torch.LongTensor).numpy())\n",
        "    optimizer.zero_grad()\n",
        "    out, att_loss = model(x, edges)\n",
        "    loss = criterion(out, y.squeeze(1).type(torch.LongTensor))\n",
        "    loss += 4.0 * att_loss\n",
        "    loss_list.append(loss)\n",
        "    pred = out.argmax(dim=1)\n",
        "    pred_list.append(pred) \n",
        "    pred_list_numpy.append(pred.numpy()) \n",
        "    loss.backward()\n",
        "    optimizer.step()  \n",
        "    \n",
        "  \n",
        "  return loss_list, pred_list, y_list, y_list_numpy, pred_list_numpy\n",
        "\n",
        "def test(model, loader):\n",
        "  model.eval()\n",
        "  pred_list, y_list, y_list_numpy = [], [], []\n",
        "  for batch in loader:\n",
        "    x, edges = batch.x, batch.edge_index\n",
        "    y = batch.y\n",
        "    y_list.append(y.squeeze(1).type(torch.LongTensor))\n",
        "    y_list_numpy.append(y.squeeze(1).type(torch.LongTensor).numpy())\n",
        "    out, _ = model(x, edges)\n",
        "    pred = out.argmax(dim=1)\n",
        "    pred_list.append(pred)    \n",
        "  \n",
        "  return pred_list, y_list, y_list_numpy \n",
        "\n",
        "def accuracy(pred_list, y_list):\n",
        "  accuracy_list = []\n",
        "  for event_id in range(len(pred_list)): # Because this works on the length of pred_list this logically surpasses the length mismatch issue\n",
        "    length_of_each_event = len(pred_list[event_id])\n",
        "    # print(length_of_each_event)\n",
        "    num_correct_list = []\n",
        "    for node_id in range(length_of_each_event):\n",
        "      p = pred_list[event_id][node_id].numpy()\n",
        "      y = y_list[event_id][node_id].numpy()\n",
        "      num_correct = p == y \n",
        "      # print(num_correct)\n",
        "      num_correct_list.append(num_correct)\n",
        "    acc = sum(num_correct_list)/length_of_each_event\n",
        "    # print(acc)\n",
        "    accuracy_list.append(acc)\n",
        "  return accuracy_list\n",
        "\n",
        "def train_on_n_epochs(n_epochs, model, optimizer, loader):\n",
        "  # running for multiple epochs\n",
        "  total_train_acc_list = []\n",
        "  total_train_loss_list, total_train_pred_list, total_train_y_list, total_train_y_list_numpy, total_train_pred_list_numpy = [], [], [], [], []\n",
        "  time_per_epoch_list = []\n",
        "  # n_epochs = 10 \n",
        "  for _ in range(n_epochs):\n",
        "    train_acc_list = []\n",
        "    starttime = timeit.default_timer()\n",
        "    train_loss_list, train_pred_list, train_y_list, train_y_list_numpy, train_pred_list_numpy = train(model, optimizer, loader)\n",
        "    time_per_epoch_list.append(timeit.default_timer() - starttime) \n",
        "    train_acc_list = accuracy(train_pred_list, train_y_list)\n",
        "    total_train_acc_list.append(train_acc_list)\n",
        "    total_train_loss_list.append(train_loss_list)\n",
        "    total_train_y_list.append(train_y_list)\n",
        "    total_train_pred_list.append(train_pred_list)\n",
        "    total_train_y_list_numpy.append(train_y_list_numpy)\n",
        "    total_train_pred_list_numpy.append(train_pred_list_numpy)\n",
        "\n",
        "  return total_train_acc_list, total_train_loss_list, total_train_pred_list, total_train_y_list, total_train_y_list_numpy, total_train_pred_list_numpy, time_per_epoch_list\n",
        "\n",
        "def evaluate_model_performace(model, loader):\n",
        "  test_pred_list, test_y_list, test_y_list_numpy = test(model, loader)\n",
        "  test_acc_list = []\n",
        "  x = accuracy(test_pred_list, test_y_list)\n",
        "  test_acc_list.append(x)\n",
        "  return test_acc_list, test_pred_list, test_y_list, test_y_list_numpy"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN3U9D1FSZKq"
      },
      "source": [
        "# def train(model, optimizer, loader):\n",
        "#   model.train()\n",
        "#   pred_list, y_list, loss_list, y_list_numpy, pred_list_numpy = [], [],[], [], []\n",
        "#   for batch in loader:\n",
        "#     x, edges = batch.x, batch.edge_index\n",
        "#     y = batch.y\n",
        "#     y_list.append(y.squeeze(1).type(torch.LongTensor))\n",
        "#     y_list_numpy.append(y.squeeze(1).type(torch.LongTensor).numpy())\n",
        "#     optimizer.zero_grad()\n",
        "#     out = model(x, edges)\n",
        "#     loss = criterion(out, y.squeeze(1).type(torch.LongTensor))\n",
        "#     loss_list.append(loss)\n",
        "#     pred = out.argmax(dim=1)\n",
        "#     pred_list.append(pred)\n",
        "#     pred_list_numpy.append(pred.numpy()) \n",
        "#     loss.backward()\n",
        "#     optimizer.step()  \n",
        "#   return loss_list, pred_list, y_list, y_list_numpy,  pred_list_numpy\n",
        "\n",
        "# def validate(model, optimizer, loader):\n",
        "#   model.train()\n",
        "#   pred_list, y_list, loss_list, y_list_numpy, pred_list_numpy = [], [],[], [], []\n",
        "#   for batch in loader:\n",
        "#     x, edges = batch.x, batch.edge_index\n",
        "#     y = batch.y\n",
        "#     y_list.append(y.squeeze(1).type(torch.LongTensor))\n",
        "#     y_list_numpy.append(y.squeeze(1).type(torch.LongTensor).numpy())\n",
        "#     optimizer.zero_grad()\n",
        "#     out = model(x, edges)\n",
        "#     loss = criterion(out, y.squeeze(1).type(torch.LongTensor))\n",
        "#     loss_list.append(loss)\n",
        "#     pred = out.argmax(dim=1)\n",
        "#     # convert tensor to numpy\n",
        "#     pred_list.append(pred) \n",
        "#     pred_list_numpy.append(pred.numpy()) \n",
        "#     loss.backward()\n",
        "#     optimizer.step()  \n",
        "#   return loss_list, pred_list, y_list, y_list_numpy, pred_list_numpy\n",
        "\n",
        "# def accuracy(pred_list, y_list):\n",
        "#   accuracy_list = []\n",
        "#   for event_id in range(len(pred_list)): # Because this works on the length of pred_list this logically surpasses the length mismatch issue\n",
        "#     length_of_each_event = len(pred_list[event_id])\n",
        "#     num_correct_list = []\n",
        "#     for node_id in range(length_of_each_event):\n",
        "#       p = pred_list[event_id][node_id].numpy()\n",
        "#       y = y_list[event_id][node_id].numpy()\n",
        "#       num_correct = p == y \n",
        "#       num_correct_list.append(num_correct)\n",
        "#     acc = sum(num_correct_list)/length_of_each_event\n",
        "#     accuracy_list.append(acc)\n",
        "#   return accuracy_list\n",
        "\n",
        "# def train_on_n_epochs(n_epochs, model, optimizer, loader):\n",
        "#   # running for multiple epochs\n",
        "#   total_train_acc_list = []\n",
        "#   total_train_loss_list, total_train_pred_list, total_train_y_list, total_train_y_list_numpy, total_train_pred_list_numpy = [], [], [], [], []\n",
        "#   time_per_epoch_list = []\n",
        "#   for _ in range(n_epochs):\n",
        "#     train_acc_list = []\n",
        "#     starttime = timeit.default_timer()\n",
        "#     train_loss_list, train_pred_list, train_y_list, train_y_list_numpy, train_pred_list_numpy = train(model, optimizer, loader)\n",
        "#     time_per_epoch_list.append(timeit.default_timer() - starttime) \n",
        "#     train_acc_list = accuracy(train_pred_list, train_y_list)\n",
        "#     total_train_acc_list.append(train_acc_list)\n",
        "#     total_train_loss_list.append(train_loss_list)\n",
        "#     total_train_y_list.append(train_y_list)\n",
        "#     total_train_pred_list.append(train_pred_list)\n",
        "#     total_train_y_list_numpy.append(train_y_list_numpy)\n",
        "#     total_train_pred_list_numpy.append(train_pred_list_numpy)\n",
        "#   return total_train_acc_list, total_train_loss_list, total_train_pred_list, total_train_y_list, total_train_y_list_numpy, total_train_pred_list_numpy, time_per_epoch_list\n",
        "\n",
        "# def test(model, loader):\n",
        "#   model.eval()\n",
        "#   pred_list, y_list = [], []\n",
        "#   y_list_numpy = []\n",
        "#   for batch in loader:\n",
        "#     x, edges = batch.x, batch.edge_index\n",
        "#     y = batch.y\n",
        "#     y_list.append(y.squeeze(1).type(torch.LongTensor))\n",
        "#     y_list_numpy.append(y.squeeze(1).type(torch.LongTensor).numpy())\n",
        "#     out = model(x, edges)\n",
        "#     pred = out.argmax(dim=1)\n",
        "#     pred_list.append(pred)    \n",
        "#   return pred_list, y_list, y_list_numpy\n",
        "\n",
        "# def evaluate_model_performace(model, loader):\n",
        "#   test_pred_list, test_y_list, test_y_list_numpy = test(model, loader)\n",
        "#   test_acc_list = []\n",
        "#   x = accuracy(test_pred_list, test_y_list)\n",
        "#   test_acc_list.append(x)\n",
        "#   return test_acc_list, test_pred_list, test_y_list, test_y_list_numpy"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txIyGPwZyW5v"
      },
      "source": [
        "# Init Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_R-g49oB7Lr"
      },
      "source": [
        "Inits models for variable and fixed size graphs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5l3ayOPWZEt"
      },
      "source": [
        "def init_superGAT_model(variable, num_layers):\n",
        "  if variable == True:\n",
        "    dataset_num_features, dataset_num_classes = set_model_constants_variable()\n",
        "  elif variable == False:\n",
        "    dataset_num_features, dataset_num_classes = set_model_constants()\n",
        "\n",
        "  if num_layers == 2:\n",
        "    model = SuperGAT2(dataset_num_features, 16,  dataset_num_classes)\n",
        "  elif num_layers == 4:\n",
        "    model = SuperGAT4(dataset_num_features, 16, dataset_num_classes)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  return dataset_num_features, dataset_num_classes, model, optimizer, criterion\n",
        "  "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VxhTyOEyUrd"
      },
      "source": [
        "def init_superGAT_hyperparam_model(variable=True, num_layers=2, hidden_channels=16, in_heads=8, out_heads=1, dropout=0.5, attention_type='MX', edge_sample_ratio=0.8):\n",
        "  dataset_num_features, dataset_num_classes = set_model_constants_variable()\n",
        "  model = SuperGAT(dataset_num_features,hidden_channels,dataset_num_classes,in_heads, out_heads, dropout, attention_type, edge_sample_ratio)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  return dataset_num_features, dataset_num_classes, model, optimizer, criterion\n",
        "  "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMPyo1-pyI1F"
      },
      "source": [
        "# Experiment Radius, KNN, Label to save values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuesCEF6znBL"
      },
      "source": [
        "def save_to_experiment_path_as_dataframe(saved_list, path, file_name, dtype):\n",
        "  df = pd.DataFrame(data=saved_list, dtype=dtype)\n",
        "  df.to_csv(path + file_name)\n",
        "  return \"Saved as \" + path + file_name"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDfqlyAg7jLT"
      },
      "source": [
        "#Radius"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqJVzgydV_cf"
      },
      "source": [
        "def experiment_radius(data_list, df_event_list, model_radius, optimizer_radius, n_epochs = 1000, path_to_experiment = '/content/'):\n",
        "    # Data loaders for radius (60:20:20)\n",
        "    train_loader, validation_loader, test_loader,  train_end_index, validate_start_index, validate_end_index, test_start_index = create_data_loaders(data_list)\n",
        "\n",
        "    # Calculate True Mass of each particle for each train/valid/test \n",
        "    train = df_event_list[:train_end_index]\n",
        "    valid = df_event_list[validate_start_index:validate_end_index]\n",
        "    test = df_event_list[test_start_index:]\n",
        "    # Split the data lists of graph events by (60:20:20)\n",
        "    data_list_train = data_list[:train_end_index]\n",
        "    data_list_valid = data_list[validate_start_index:validate_end_index]\n",
        "    data_list_test = data_list[test_start_index:]\n",
        "    # Calculate true mass\n",
        "    total_true_mass_H_list_train, total_true_mass_Z_list_train, total_true_mass_O_list_train = get_true_mass_list(data_list_train, train, len(train))\n",
        "    total_true_mass_H_list_valid, total_true_mass_Z_list_valid, total_true_mass_O_list_valid = get_true_mass_list(data_list_valid, valid, len(valid))\n",
        "    total_true_mass_H_list_test, total_true_mass_Z_list_test, total_true_mass_O_list_test = get_true_mass_list(data_list_test, test, len(test))\n",
        "\n",
        "    # Training on Radius Dataset\n",
        "    total_train_acc_list, total_train_loss_list, total_train_pred_list, total_train_y_list, total_train_y_list_numpy, total_train_pred_list_numpy, train_time_per_epoch_list = train_on_n_epochs(n_epochs, model_radius, optimizer_radius, train_loader)\n",
        "    total_valid_acc_list, total_valid_loss_list, total_valid_pred_list, total_valid_y_list, total_valid_y_list_numpy, total_valid_pred_list_numpy, valid_time_per_epoch_list = train_on_n_epochs(n_epochs, model_radius, optimizer_radius, validation_loader)\n",
        "    mean_train_epoch_time = sum(train_time_per_epoch_list)/len(train_time_per_epoch_list)\n",
        "    mean_valid_epoch_time = sum(valid_time_per_epoch_list)/len(valid_time_per_epoch_list)\n",
        "\n",
        "    # Testing on Radius Data set \n",
        "    test_acc_list, test_pred_list, test_y_list, test_y_list_numpy = evaluate_model_performace(model_radius, test_loader)\n",
        "\n",
        "    # Calculate the set of predicted mass for each particle\n",
        "    total_pred_mass_H_list_train, total_pred_mass_Z_list_train, total_pred_mass_O_list_train = calculate_predicted_mass(data_list_train, train, total_train_pred_list[-1], len(train))\n",
        "    total_pred_mass_H_list_valid, total_pred_mass_Z_list_valid, total_pred_mass_O_list_valid = calculate_predicted_mass(data_list_valid, valid, total_valid_pred_list[-1], len(valid))\n",
        "    total_pred_mass_H_list_test, total_pred_mass_Z_list_test, total_pred_mass_O_list_test = calculate_predicted_mass(data_list_test, test, test_pred_list, len(test))\n",
        "\n",
        "    # Results\n",
        "    train_acc_radius = ['radius_train', np.mean(total_train_acc_list[-1]),np.std(total_train_acc_list[-1])]\n",
        "    valid_acc_radius = ['radius_valid', np.mean(total_valid_acc_list[-1]),np.std(total_valid_acc_list[-1])]\n",
        "    test_acc_radius = ['radius_test', np.mean(test_acc_list), np.std(test_acc_list) ]\n",
        "    df_result_radius = pd.DataFrame([train_acc_radius, valid_acc_radius, test_acc_radius ], columns=['type','mean', 'stdev'])\n",
        "\n",
        "    # save to drive \n",
        "    df_result_radius.to_csv(path_to_experiment  + 'df_result_radius.csv' )\n",
        "    print(df_result_radius)\n",
        "\n",
        "    # fix file names \n",
        "    file_name1 = 'df_radius_train_acc.csv'\n",
        "    file_name2 = 'df_radius_test_acc.csv'\n",
        "    file_name3 = 'df_radius_train_loss.csv'\n",
        "    file_name4 = 'df_radius_test_pred.csv'\n",
        "    file_name5 = 'df_radius_test_y.csv'\n",
        "    file_name6 = 'df_radius_train_pred.csv'\n",
        "    file_name7 = 'df_radius_train_y.csv'\n",
        "    file_name8 = 'df_radius_train_time_per_epoch_list.csv'\n",
        "    file_name9 = 'df_radius_valid_time_per_epoch_list.csv'\n",
        "    file_name10 = 'df_radius_valid_acc.csv'\n",
        "    file_name11 = 'df_radius_valid_loss.csv'\n",
        "    file_name12 = 'df_radius_valid_pred.csv'\n",
        "    file_name13 = 'df_radius_valid_y.csv'\n",
        "    file_name14 = 'df_radius_test_y_numpy.csv'\n",
        "    file_name15 = 'df_radius_train_y_numpy.csv'\n",
        "    file_name16 = 'df_radius_valid_y_numpy.csv'\n",
        "    file_name17 = \"df_radius_true_mass_H_list_train\"\n",
        "    file_name18 = \"df_radius_true_mass_Z_list_train\"\n",
        "    file_name19 = \"df_radius_true_mass_O_list_train\"\n",
        "    file_name20 = \"df_radius_true_mass_H_list_valid\"\n",
        "    file_name21 = \"df_radius_true_mass_Z_list_valid\"\n",
        "    file_name22 = \"df_radius_true_mass_O_list_valid\"\n",
        "    file_name23 = \"df_radius_true_mass_H_list_test\"\n",
        "    file_name24 = \"df_radius_true_mass_Z_list_test\"\n",
        "    file_name25 = \"df_radius_true_mass_O_list_test\"\n",
        "    file_name26 = \"df_radius_pred_mass_H_list_train\"\n",
        "    file_name27 = \"df_radius_pred_mass_Z_list_train\"\n",
        "    file_name28 = \"df_radius_pred_mass_O_list_train\"\n",
        "    file_name29 = \"df_radius_pred_mass_H_list_valid\"\n",
        "    file_name30 = \"df_radius_pred_mass_Z_list_valid\"\n",
        "    file_name31 = \"df_radius_pred_mass_O_list_valid\"\n",
        "    file_name32 = \"df_radius_pred_mass_H_list_test\"\n",
        "    file_name33 = \"df_radius_pred_mass_Z_list_test\"\n",
        "    file_name34 = \"df_radius_pred_mass_O_list_test\"\n",
        "    file_name35 = 'df_radius_train_pred_numpy.csv'\n",
        "    file_name36 = 'df_radius_valid_pred_numpy.csv'\n",
        "\n",
        "    # use path_to_experiment\n",
        "    save_to_experiment_path_as_dataframe(total_train_acc_list,  path_to_experiment, file_name1, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(test_acc_list, path_to_experiment, file_name2, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_train_loss_list, path_to_experiment, file_name3, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(test_pred_list, path_to_experiment, file_name4, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(test_y_list, path_to_experiment, file_name5, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(total_train_pred_list, path_to_experiment, file_name6, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(total_train_y_list, path_to_experiment, file_name7, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(train_time_per_epoch_list, path_to_experiment, file_name8, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(valid_time_per_epoch_list, path_to_experiment, file_name9, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_acc_list,  path_to_experiment, file_name10, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_loss_list, path_to_experiment, file_name11, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_pred_list, path_to_experiment, file_name12, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_y_list, path_to_experiment, file_name13, dtype=object)\n",
        "    # test y labels \n",
        "    save_to_experiment_path_as_dataframe(test_y_list_numpy, path_to_experiment, file_name14, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_train_y_list_numpy, path_to_experiment, file_name15, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_y_list_numpy, path_to_experiment, file_name16, dtype=float)\n",
        "    # mass true\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_H_list_train,  path_to_experiment, file_name17, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_Z_list_train,  path_to_experiment, file_name18, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_O_list_train,  path_to_experiment, file_name19, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_H_list_valid,  path_to_experiment, file_name20, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_Z_list_valid,  path_to_experiment, file_name21, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_O_list_valid,  path_to_experiment, file_name22, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_H_list_test,  path_to_experiment, file_name23, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_Z_list_test,  path_to_experiment, file_name24, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_O_list_test,  path_to_experiment, file_name25, dtype=float)\n",
        "\n",
        "    # mass calculated\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_H_list_train,  path_to_experiment, file_name26, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_Z_list_train,  path_to_experiment, file_name27, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_O_list_train,  path_to_experiment, file_name28, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_H_list_valid,  path_to_experiment, file_name29, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_Z_list_valid,  path_to_experiment, file_name30, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_O_list_valid,  path_to_experiment, file_name31, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_H_list_test,  path_to_experiment, file_name32, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_Z_list_test,  path_to_experiment, file_name33, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_O_list_test,  path_to_experiment, file_name34, dtype=float)\n",
        "    # numpy pred labels \n",
        "    save_to_experiment_path_as_dataframe(total_train_pred_list_numpy, path_to_experiment, file_name35, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_pred_list_numpy, path_to_experiment, file_name36, dtype=float)\n",
        "   \n",
        "\n",
        "    # Save Model \n",
        "    torch.save(model_radius.state_dict(), path_to_experiment + 'model_radius.pickle')\n",
        "    return mean_train_epoch_time, mean_valid_epoch_time, df_result_radius\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckbp74ymofqX"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c19HXeBaSfBK"
      },
      "source": [
        "def experiment_knn(data_list, df_event_list, model_knn, optimizer_knn, n_epochs = 1000, path_to_experiment = '/content/'):\n",
        "    # Data loaders for KNN (60:20:20)\n",
        "    train_loader, validation_loader, test_loader,  train_end_index, validate_start_index, validate_end_index, test_start_index = create_data_loaders(data_list)\n",
        "\n",
        "    # Calculate True Mass of each particle for each train/valid/test \n",
        "    train = df_event_list[:train_end_index]\n",
        "    valid = df_event_list[validate_start_index:validate_end_index]\n",
        "    test = df_event_list[test_start_index:]\n",
        "    # Split the data lists of graph events by (60:20:20)\n",
        "    data_list_train = data_list[:train_end_index]\n",
        "    data_list_valid = data_list[validate_start_index:validate_end_index]\n",
        "    data_list_test = data_list[test_start_index:]\n",
        "    # Call method over the 3 lists \n",
        "    total_true_mass_H_list_train, total_true_mass_Z_list_train, total_true_mass_O_list_train = get_true_mass_list(data_list_train, train, len(train))\n",
        "    total_true_mass_H_list_valid, total_true_mass_Z_list_valid, total_true_mass_O_list_valid = get_true_mass_list(data_list_valid, valid, len(valid))\n",
        "    total_true_mass_H_list_test, total_true_mass_Z_list_test, total_true_mass_O_list_test = get_true_mass_list(data_list_test, test, len(test))\n",
        "\n",
        "    # Training on KNN Dataset\n",
        "    total_train_acc_list, total_train_loss_list, total_train_pred_list, total_train_y_list, total_train_y_list_numpy, total_train_pred_list_numpy, train_time_per_epoch_list = train_on_n_epochs(n_epochs, model_knn, optimizer_knn, train_loader)\n",
        "    total_valid_acc_list, total_valid_loss_list, total_valid_pred_list, total_valid_y_list, total_valid_y_list_numpy, total_valid_pred_list_numpy, valid_time_per_epoch_list = train_on_n_epochs(n_epochs, model_knn, optimizer_knn, validation_loader)\n",
        "    mean_train_epoch_time = sum(train_time_per_epoch_list)/len(train_time_per_epoch_list)\n",
        "    mean_valid_epoch_time = sum(valid_time_per_epoch_list)/len(valid_time_per_epoch_list)\n",
        "\n",
        "    # Testing on KNN Data set \n",
        "    test_acc_list, test_pred_list, test_y_list, test_y_list_numpy = evaluate_model_performace(model_knn, test_loader)\n",
        "    \n",
        "    # Calculate the set of predicted mass for each particle\n",
        "    total_pred_mass_H_list_train, total_pred_mass_Z_list_train, total_pred_mass_O_list_train = calculate_predicted_mass(data_list_train, train, total_train_pred_list[-1], len(train))\n",
        "    total_pred_mass_H_list_valid, total_pred_mass_Z_list_valid, total_pred_mass_O_list_valid = calculate_predicted_mass(data_list_valid, valid, total_valid_pred_list[-1], len(valid))\n",
        "    total_pred_mass_H_list_test, total_pred_mass_Z_list_test, total_pred_mass_O_list_test = calculate_predicted_mass(data_list_test, test, test_pred_list, len(test))\n",
        "\n",
        "    # Results\n",
        "    train_acc_knn = ['knn_train', np.mean(total_train_acc_list[-1]),np.std(total_train_acc_list[-1])]\n",
        "    valid_acc_knn = ['knn_valid', np.mean(total_valid_acc_list[-1]),np.std(total_valid_acc_list[-1])]\n",
        "    test_acc_knn = ['knn_test', np.mean(test_acc_list), np.std(test_acc_list) ]\n",
        "\n",
        "    df_result_knn = pd.DataFrame([train_acc_knn, valid_acc_knn, test_acc_knn ], columns=['type','mean', 'stdev'])\n",
        "\n",
        "    # save to drive \n",
        "    df_result_knn.to_csv(path_to_experiment  + 'df_result_knn.csv' )\n",
        "    print(df_result_knn)\n",
        "\n",
        "    # fix file names \n",
        "    file_name1 = 'df_knn_train_acc.csv'\n",
        "    file_name2 = 'df_knn_test_acc.csv'\n",
        "    file_name3 = 'df_knn_train_loss.csv'\n",
        "    file_name4 = 'df_knn_test_pred.csv'\n",
        "    file_name5 = 'df_knn_test_y.csv'\n",
        "    file_name6 = 'df_knn_train_pred.csv'\n",
        "    file_name7 = 'df_knn_train_y.csv'\n",
        "    file_name8 = 'df_knn_train_time_per_epoch_list.csv'\n",
        "    file_name9 = 'df_knn_valid_time_per_epoch_list.csv'\n",
        "    file_name10 = 'df_knn_valid_acc.csv'\n",
        "    file_name11 = 'df_knn_valid_loss.csv'\n",
        "    file_name12 = 'df_knn_valid_pred.csv'\n",
        "    file_name13 = 'df_knn_valid_y.csv'\n",
        "    file_name14 = 'df_knn_test_y_numpy.csv'\n",
        "    file_name15 = 'df_knn_train_y_numpy.csv'\n",
        "    file_name16 = 'df_knn_valid_y_numpy.csv'\n",
        "    file_name17 = \"df_knn_true_mass_H_list_train\"\n",
        "    file_name18 = \"df_knn_true_mass_Z_list_train\"\n",
        "    file_name19 = \"df_knn_true_mass_O_list_train\"\n",
        "    file_name20 = \"df_knn_true_mass_H_list_valid\"\n",
        "    file_name21 = \"df_knn_true_mass_Z_list_valid\"\n",
        "    file_name22 = \"df_knn_true_mass_O_list_valid\"\n",
        "    file_name23 = \"df_knn_true_mass_H_list_test\"\n",
        "    file_name24 = \"df_knn_true_mass_Z_list_test\"\n",
        "    file_name25 = \"df_knn_true_mass_O_list_test\"\n",
        "    file_name26 = \"df_knn_pred_mass_H_list_train\"\n",
        "    file_name27 = \"df_knn_pred_mass_Z_list_train\"\n",
        "    file_name28 = \"df_knn_pred_mass_O_list_train\"\n",
        "    file_name29 = \"df_knn_pred_mass_H_list_valid\"\n",
        "    file_name30 = \"df_knn_pred_mass_Z_list_valid\"\n",
        "    file_name31 = \"df_knn_pred_mass_O_list_valid\"\n",
        "    file_name32 = \"df_knn_pred_mass_H_list_test\"\n",
        "    file_name33 = \"df_knn_pred_mass_Z_list_test\"\n",
        "    file_name34 = \"df_knn_pred_mass_O_list_test\"\n",
        "    file_name35 = 'df_knn_train_pred_numpy.csv'\n",
        "    file_name36 = 'df_knn_valid_pred_numpy.csv'\n",
        "\n",
        "    # use path_to_experiment\n",
        "    save_to_experiment_path_as_dataframe(total_train_acc_list,  path_to_experiment, file_name1, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(test_acc_list, path_to_experiment, file_name2, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_train_loss_list, path_to_experiment, file_name3, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(test_pred_list, path_to_experiment, file_name4, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(test_y_list, path_to_experiment, file_name5, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(total_train_pred_list, path_to_experiment, file_name6, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(total_train_y_list, path_to_experiment, file_name7, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(train_time_per_epoch_list, path_to_experiment, file_name8, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(valid_time_per_epoch_list, path_to_experiment, file_name9, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_acc_list,  path_to_experiment, file_name10, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_loss_list, path_to_experiment, file_name11, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_pred_list, path_to_experiment, file_name12, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_y_list, path_to_experiment, file_name13, dtype=object)\n",
        "    # test y labels \n",
        "    save_to_experiment_path_as_dataframe(test_y_list_numpy, path_to_experiment, file_name14, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_train_y_list_numpy, path_to_experiment, file_name15, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_y_list_numpy, path_to_experiment, file_name16, dtype=float)\n",
        "    # mass true\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_H_list_train,  path_to_experiment, file_name17, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_Z_list_train,  path_to_experiment, file_name18, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_O_list_train,  path_to_experiment, file_name19, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_H_list_valid,  path_to_experiment, file_name20, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_Z_list_valid,  path_to_experiment, file_name21, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_O_list_valid,  path_to_experiment, file_name22, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_H_list_test,  path_to_experiment, file_name23, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_Z_list_test,  path_to_experiment, file_name24, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_O_list_test,  path_to_experiment, file_name25, dtype=float)\n",
        "\n",
        "    # mass calculated\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_H_list_train,  path_to_experiment, file_name26, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_Z_list_train,  path_to_experiment, file_name27, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_O_list_train,  path_to_experiment, file_name28, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_H_list_valid,  path_to_experiment, file_name29, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_Z_list_valid,  path_to_experiment, file_name30, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_O_list_valid,  path_to_experiment, file_name31, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_H_list_test,  path_to_experiment, file_name32, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_Z_list_test,  path_to_experiment, file_name33, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_O_list_test,  path_to_experiment, file_name34, dtype=float)\n",
        "    # numpy pred labels \n",
        "    save_to_experiment_path_as_dataframe(total_train_pred_list_numpy, path_to_experiment, file_name35, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_pred_list_numpy, path_to_experiment, file_name36, dtype=float)\n",
        "\n",
        "    # Save model \n",
        "    torch.save(model_knn.state_dict(), path_to_experiment + 'model_knn.pickle')\n",
        "    return mean_train_epoch_time, mean_valid_epoch_time, df_result_knn"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I0k-E1D7nMw"
      },
      "source": [
        "# Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrJzNLd5W7Nr"
      },
      "source": [
        "def experiment_label(data_list, df_event_list, model_label, optimizer_label, n_epochs = 1000, path_to_experiment = '/content/'):\n",
        "\n",
        "    # Data loaders for label (60:20:20)\n",
        "    train_loader, validation_loader, test_loader,  train_end_index, validate_start_index, validate_end_index, test_start_index = create_data_loaders(data_list)\n",
        "\n",
        "    # Calculate True Mass of each particle for each train/valid/test \n",
        "    # Split the dataframe based on the (60:20:20)\n",
        "    train = df_event_list[:train_end_index]\n",
        "    valid = df_event_list[validate_start_index:validate_end_index]\n",
        "    test = df_event_list[test_start_index:]\n",
        "    # Split the data lists of graph events by (60:20:20)\n",
        "    data_list_train = data_list[:train_end_index]\n",
        "    data_list_valid = data_list[validate_start_index:validate_end_index]\n",
        "    data_list_test = data_list[test_start_index:]\n",
        "    # Call method to calculate true mass over the 3 lists \n",
        "    total_true_mass_H_list_train, total_true_mass_Z_list_train, total_true_mass_O_list_train = get_true_mass_list(data_list_train, train, len(train))\n",
        "    total_true_mass_H_list_valid, total_true_mass_Z_list_valid, total_true_mass_O_list_valid = get_true_mass_list(data_list_valid, valid, len(valid))\n",
        "    total_true_mass_H_list_test, total_true_mass_Z_list_test, total_true_mass_O_list_test = get_true_mass_list(data_list_test, test, len(test))\n",
        "\n",
        "    # Training on Label Dataset\n",
        "    total_train_acc_list, total_train_loss_list, total_train_pred_list, total_train_y_list, total_train_y_list_numpy,total_train_pred_list_numpy, train_time_per_epoch_list = train_on_n_epochs(n_epochs, model_label, optimizer_label, train_loader)\n",
        "    total_valid_acc_list, total_valid_loss_list, total_valid_pred_list, total_valid_y_list, total_valid_y_list_numpy,total_valid_pred_list_numpy, valid_time_per_epoch_list = train_on_n_epochs(n_epochs, model_label, optimizer_label, validation_loader)\n",
        "    mean_train_epoch_time = sum(train_time_per_epoch_list)/len(train_time_per_epoch_list)\n",
        "    mean_valid_epoch_time = sum(valid_time_per_epoch_list)/len(valid_time_per_epoch_list)\n",
        "\n",
        "    # Testing on Label Data set \n",
        "    test_acc_list, test_pred_list, test_y_list, test_y_list_numpy = evaluate_model_performace(model_label, test_loader)\n",
        "\n",
        "\n",
        "    # Calculate the set of predicted mass for each particle\n",
        "    total_pred_mass_H_list_train, total_pred_mass_Z_list_train, total_pred_mass_O_list_train = calculate_predicted_mass(data_list_train, train, total_train_pred_list[-1], len(train))\n",
        "    total_pred_mass_H_list_valid, total_pred_mass_Z_list_valid, total_pred_mass_O_list_valid = calculate_predicted_mass(data_list_valid, valid, total_valid_pred_list[-1], len(valid))\n",
        "    total_pred_mass_H_list_test, total_pred_mass_Z_list_test, total_pred_mass_O_list_test = calculate_predicted_mass(data_list_test, test, test_pred_list, len(test))\n",
        "    \n",
        "\n",
        "    # Results\n",
        "    train_acc_label = ['label_train', np.mean(total_train_acc_list[-1]),np.std(total_train_acc_list[-1])]\n",
        "    valid_acc_label = ['label_valid', np.mean(total_valid_acc_list[-1]),np.std(total_valid_acc_list[-1])]\n",
        "    test_acc_label = ['label_test', np.mean(test_acc_list), np.std(test_acc_list) ]\n",
        "\n",
        "    df_result_label = pd.DataFrame([train_acc_label, valid_acc_label, test_acc_label ], columns=['type','mean', 'stdev'])\n",
        "\n",
        "    # save to drive \n",
        "    df_result_label.to_csv(path_to_experiment  + 'df_result_label.csv' )\n",
        "    print(df_result_label)\n",
        "\n",
        "    # fix file names \n",
        "    file_name1 = 'df_label_train_acc.csv'\n",
        "    file_name2 = 'df_label_test_acc.csv'\n",
        "    file_name3 = 'df_label_train_loss.csv'\n",
        "    file_name4 = 'df_label_test_pred.csv'\n",
        "    file_name5 = 'df_label_test_y.csv'\n",
        "    file_name6 = 'df_label_train_pred.csv'\n",
        "    file_name7 = 'df_label_train_y.csv'\n",
        "    file_name8 = 'df_label_train_time_per_epoch_list.csv'\n",
        "    file_name9 = 'df_label_valid_time_per_epoch_list.csv'\n",
        "    file_name10 = 'df_label_valid_acc.csv'\n",
        "    file_name11 = 'df_label_valid_loss.csv'\n",
        "    file_name12 = 'df_label_valid_pred.csv'\n",
        "    file_name13 = 'df_label_valid_y.csv'\n",
        "    file_name14 = 'df_label_test_y_numpy.csv'\n",
        "    file_name15 = 'df_label_train_y_numpy.csv'\n",
        "    file_name16 = 'df_label_valid_y_numpy.csv'\n",
        "    file_name17 = \"df_label_true_mass_H_list_train\"\n",
        "    file_name18 = \"df_label_true_mass_Z_list_train\"\n",
        "    file_name19 = \"df_label_true_mass_O_list_train\"\n",
        "    file_name20 = \"df_label_true_mass_H_list_valid\"\n",
        "    file_name21 = \"df_label_true_mass_Z_list_valid\"\n",
        "    file_name22 = \"df_label_true_mass_O_list_valid\"\n",
        "    file_name23 = \"df_label_true_mass_H_list_test\"\n",
        "    file_name24 = \"df_label_true_mass_Z_list_test\"\n",
        "    file_name25 = \"df_label_true_mass_O_list_test\"\n",
        "    file_name26 = \"df_label_pred_mass_H_list_train\"\n",
        "    file_name27 = \"df_label_pred_mass_Z_list_train\"\n",
        "    file_name28 = \"df_label_pred_mass_O_list_train\"\n",
        "    file_name29 = \"df_label_pred_mass_H_list_valid\"\n",
        "    file_name30 = \"df_label_pred_mass_Z_list_valid\"\n",
        "    file_name31 = \"df_label_pred_mass_O_list_valid\"\n",
        "    file_name32 = \"df_label_pred_mass_H_list_test\"\n",
        "    file_name33 = \"df_label_pred_mass_Z_list_test\"\n",
        "    file_name34 = \"df_label_pred_mass_O_list_test\"\n",
        "    file_name35 = 'df_label_train_pred_numpy.csv'\n",
        "    file_name36 = 'df_label_valid_pred_numpy.csv'\n",
        "\n",
        "    # use path_to_experiment\n",
        "    save_to_experiment_path_as_dataframe(total_train_acc_list,  path_to_experiment, file_name1, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(test_acc_list, path_to_experiment, file_name2, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_train_loss_list, path_to_experiment, file_name3, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(test_pred_list, path_to_experiment, file_name4, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(test_y_list, path_to_experiment, file_name5, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(total_train_pred_list, path_to_experiment, file_name6, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(total_train_y_list, path_to_experiment, file_name7, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(train_time_per_epoch_list, path_to_experiment, file_name8, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(valid_time_per_epoch_list, path_to_experiment, file_name9, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_acc_list,  path_to_experiment, file_name10, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_loss_list, path_to_experiment, file_name11, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_pred_list, path_to_experiment, file_name12, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_y_list, path_to_experiment, file_name13, dtype=object)\n",
        "    # test y labels \n",
        "    save_to_experiment_path_as_dataframe(test_y_list_numpy, path_to_experiment, file_name14, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_train_y_list_numpy, path_to_experiment, file_name15, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_y_list_numpy, path_to_experiment, file_name16, dtype=float)\n",
        "    # mass true\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_H_list_train,  path_to_experiment, file_name17, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_Z_list_train,  path_to_experiment, file_name18, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_O_list_train,  path_to_experiment, file_name19, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_H_list_valid,  path_to_experiment, file_name20, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_Z_list_valid,  path_to_experiment, file_name21, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_O_list_valid,  path_to_experiment, file_name22, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_H_list_test,  path_to_experiment, file_name23, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_Z_list_test,  path_to_experiment, file_name24, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_O_list_test,  path_to_experiment, file_name25, dtype=float)\n",
        "\n",
        "    # mass calculated\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_H_list_train,  path_to_experiment, file_name26, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_Z_list_train,  path_to_experiment, file_name27, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_O_list_train,  path_to_experiment, file_name28, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_H_list_valid,  path_to_experiment, file_name29, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_Z_list_valid,  path_to_experiment, file_name30, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_O_list_valid,  path_to_experiment, file_name31, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_H_list_test,  path_to_experiment, file_name32, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_Z_list_test,  path_to_experiment, file_name33, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_O_list_test,  path_to_experiment, file_name34, dtype=float)\n",
        "    # numpy pred labels \n",
        "    save_to_experiment_path_as_dataframe(total_train_pred_list_numpy, path_to_experiment, file_name35, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_pred_list_numpy, path_to_experiment, file_name36, dtype=float)\n",
        "\n",
        "    # Save model to path \n",
        "    torch.save(model_label.state_dict(), path_to_experiment + 'model_label.pickle')\n",
        "\n",
        "    return mean_train_epoch_time, mean_valid_epoch_time, df_result_label"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOrAVwgCmrqJ"
      },
      "source": [
        "#Create Graphs - \n",
        "mention FILE NAME and TOTAL EVENTS to be processed and MAX EVENT LENGTh for fixed graphs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzDNqDGVmqlH"
      },
      "source": [
        "# # MENTION FILE NAME Fixed Graph - Create the 3 lists that will be passed through the data loaders for processing\n",
        "# # path_to_file=\"output_11_07_2021.csv\"\n",
        "# path_to_file = '/content/drive/MyDrive/FCC_Experiments_2021/dataset/processed_csv_files/output_11_07_2021.csv'\n",
        "\n",
        "# data_radius_list, data_knn_list, data_label_list, df_event_list, length_of_each_event, df_event_processed_list_cleaned = get_data_lists_for_fixed_size_graphs(path_to_file, TOTAL_EVENTS=10000, MAX_LENGTH_EVENT=150)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm7MGDdXAFIZ"
      },
      "source": [
        "# MENTION FILE NAME Variable Graph - Create the 3 lists that will be passed through the data loaders for processing\n",
        "# path_to_file=\"output_11_07_2021.csv\"\n",
        "path_to_file = '/content/drive/MyDrive/FCC_Experiments_2021/dataset/processed_csv_files/output_11_07_2021.csv'\n",
        "data_radius_list_variable, data_knn_list_variable, data_label_list_variable, df_event_list_variable, length_of_each_event_variable, df_event_processed_list_cleaned_variable = get_data_lists_for_variable_size_graphs(path_to_file, TOTAL_EVENTS=10000)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0IlYtohC98x"
      },
      "source": [
        "# # MENTION FILE NAME Variable Graph - only H and Z events - WILL PROCESS 10 k events at the moment \n",
        "# data_radius_list_variable_H_Z, data_knn_list_variable_H_Z, data_label_list_variable_H_Z, df_event_list_variable_H_Z, length_of_each_event_variable_H_Z, df_event_processed_list_cleaned_variable_H_Z, TOTAL_EVENTS =  get_data_lists_for_variable_size_graphs_H_Z(path_to_file=\"/content/output_11_07_2021.csv\", path_to_file_fixed_columns='/content/fixed_column.csv', TOTAL_EVENTS=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3iRiyjQCZhg"
      },
      "source": [
        "# superGATX\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ta0EIxMnC4J"
      },
      "source": [
        "#Init models for variable sized graph "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwUs-yvrQoLu"
      },
      "source": [
        "## superGAT 2 layer - MX Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql9w_mqQnB92"
      },
      "source": [
        "dataset_num_features, dataset_num_classes, model_knn_variable_depth_2_supergat, optimizer_knn_variable_depth_2_supergat, criterion= init_superGAT_hyperparam_model(variable=True, num_layers=2, hidden_channels=16, in_heads=8, out_heads=1, dropout=0.5, attention_type='MX', edge_sample_ratio=0.8)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJeOOCdtnGuk",
        "outputId": "d8e86d71-8238-48fa-82ed-7485e0380873"
      },
      "source": [
        "path_to_experiment_variable_depth_2_supergat_knn = '/content/drive/MyDrive/FCC_Experiments_2021/model_supergat/variable/depth_2/head_8_attn_MX_edge_0.8/knn/'\n",
        "train_epoch_knn_2, valid_epoch_knn_2, df_result_knn_2 = experiment_knn(data_knn_list_variable, df_event_processed_list_cleaned_variable, model_knn_variable_depth_2_supergat, optimizer_knn_variable_depth_2_supergat, n_epochs = 51, path_to_experiment = path_to_experiment_variable_depth_2_supergat_knn)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        type      mean     stdev\n",
            "0  knn_train  0.963174  0.021821\n",
            "1  knn_valid  0.964493  0.019185\n",
            "2   knn_test  0.958934  0.011708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQteXIzD3jGI"
      },
      "source": [
        "## superGAT 2 layer - SD Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khiB_rHn3dGo",
        "outputId": "848c8dfa-cd47-4b30-e79b-a18107aeb53e"
      },
      "source": [
        "dataset_num_features, dataset_num_classes, model_knn_variable_depth_2_supergat, optimizer_knn_variable_depth_2_supergat, criterion= init_superGAT_hyperparam_model(variable=True, num_layers=2, hidden_channels=16, in_heads=8, out_heads=1, dropout=0.5, attention_type='SD', edge_sample_ratio=0.8)\n",
        "path_to_experiment_variable_depth_2_supergat_knn = '/content/drive/MyDrive/FCC_Experiments_2021/model_supergat/variable/depth_2/head_8_attn_SD_edge_0.8/knn/'\n",
        "train_epoch_knn_2, valid_epoch_knn_2, df_result_knn_2 = experiment_knn(data_knn_list_variable, df_event_processed_list_cleaned_variable, model_knn_variable_depth_2_supergat, optimizer_knn_variable_depth_2_supergat, n_epochs = 51, path_to_experiment = path_to_experiment_variable_depth_2_supergat_knn)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        type      mean     stdev\n",
            "0  knn_train  0.962202  0.023008\n",
            "1  knn_valid  0.962963  0.020663\n",
            "2   knn_test  0.958934  0.011708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RkFGRwP5xMJ",
        "outputId": "c2039f0e-d554-4521-cc6e-7c7b64c45c60"
      },
      "source": [
        "dataset_num_features, dataset_num_classes, model_knn_variable_depth_2_supergat, optimizer_knn_variable_depth_2_supergat, criterion= init_superGAT_hyperparam_model(variable=True, num_layers=2, hidden_channels=16, in_heads=1, out_heads=1, dropout=0.5, attention_type='SD', edge_sample_ratio=1.0)\n",
        "path_to_experiment_variable_depth_2_supergat_knn = '/content/drive/MyDrive/FCC_Experiments_2021/model_supergat/variable/depth_2/head_1_attn_SD_edge_1.0/knn/'\n",
        "train_epoch_knn_2, valid_epoch_knn_2, df_result_knn_2 = experiment_knn(data_knn_list_variable, df_event_processed_list_cleaned_variable, model_knn_variable_depth_2_supergat, optimizer_knn_variable_depth_2_supergat, n_epochs = 51, path_to_experiment = path_to_experiment_variable_depth_2_supergat_knn)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        type      mean     stdev\n",
            "0  knn_train  0.961735  0.021853\n",
            "1  knn_valid  0.962669  0.019361\n",
            "2   knn_test  0.958934  0.011708\n"
          ]
        }
      ]
    }
  ]
}
