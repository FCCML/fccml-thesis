{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fcc_experiment_2.x_model_GCNX_2_hyperparameters_dropout_0.1_0.2_0.3_0.4_0.5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOhEUZ7+rTPiLcM5xu46ANP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akanksha-ahuja/fcc-final-notebooks/blob/main/fcc_experiment_2_x_model_GCNX_2_hyperparameters_dropout_0_1_0_2_0_3_0_4_0_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEvtMEMbC7JA"
      },
      "source": [
        "# Import libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b6_CQhONZUK"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "import timeit\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import distance\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IcwbaY7NkWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df476164-fada-4c51-d658-f6c9652e2c35"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install graphlime\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from graphlime import GraphLIME\n",
        "from torch_geometric.utils import to_networkx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.0 MB 348 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 242 kB/s \n",
            "\u001b[K     |████████████████████████████████| 222 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 376 kB 35.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.3 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting graphlime\n",
            "  Downloading graphlime-1.2.0.tar.gz (3.3 kB)\n",
            "Building wheels for collected packages: graphlime\n",
            "  Building wheel for graphlime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphlime: filename=graphlime-1.2.0-py3-none-any.whl size=2616 sha256=ccc1c1d83f8df60c35cff0a883e2effde1987ab111cea483c56e119af98d948d\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/29/94/9835c557e2def18b58369cda0032935a3263acfa9266aaeb5d\n",
            "Successfully built graphlime\n",
            "Installing collected packages: graphlime\n",
            "Successfully installed graphlime-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHW1SAYqy9an"
      },
      "source": [
        "from torch_geometric.nn import GCNConv, TAGConv, SAGEConv, ChebConv\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.nn import GINConv\n",
        "from torch_geometric.nn import JumpingKnowledge, GCN2Conv\n",
        "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
        "from torch_geometric.nn import GNNExplainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WR3_jqNCvyx"
      },
      "source": [
        "# Connect G drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dsc5R9Z8CyV2",
        "outputId": "f270becc-db72-4a4a-d085-ecee4701c0fc"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k82Fl5gRUE3s"
      },
      "source": [
        "# Load df and process the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0erGxUrnNne9"
      },
      "source": [
        "# Data Processing Functions\n",
        "def load_df(path_to_file):\n",
        "  df = pd.read_csv(path_to_file)\n",
        "  return df\n",
        "\n",
        "def set_constants(TOTAL_EVENTS, MAX_LENGTH_EVENT=150):\n",
        "  TOTAL_EVENTS = TOTAL_EVENTS\n",
        "  MAX_LENGTH_EVENT = MAX_LENGTH_EVENT\n",
        "  return TOTAL_EVENTS, MAX_LENGTH_EVENT\n",
        "\n",
        "def create_labels(df):\n",
        "  conditions = [(df['isHiggs'] == True),(df['isZ'] == True), (df['isOther'] == True) ]\n",
        "  # create a list of the values we want to assign for each condition\n",
        "  values = [0, 1, 2] \n",
        "\n",
        "  # create a new column and use np.select to assign values to it using our lists as arguments\n",
        "  df['label'] = np.select(conditions, values)\n",
        "  return df\n",
        "\n",
        "\n",
        "def normalise_x_features(df):\n",
        "  # Normalise the features in the dataset \n",
        "  df_id = df[['event_list']]\n",
        "  df_x = df[['pid', 'pos_r', 'pos_theta', 'pos_phi', 'pos_t', 'mom_p', 'mom_theta', 'mom_phi', 'mom_mass']]\n",
        "  df_y = df[['label']]\n",
        "\n",
        "  # Create a list of labels for the new dataframe\n",
        "  new_columns = ['event_list', 'pid', 'pos_r', 'pos_theta', 'pos_phi', 'pos_t', 'mom_p', 'mom_theta', 'mom_phi', 'mom_mass', 'label']\n",
        "\n",
        "  x = df_x.values # returns numpy \n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  df_x = pd.DataFrame(x_scaled)\n",
        "\n",
        "  # Concatenate normalised x features and un-normalised y labels and event ids\n",
        "  df_normalised_features = pd.concat([df_id, df_x, df_y], axis=1)\n",
        "  df_normalised_features.columns = new_columns # You need to mention the axis\n",
        "  return df_normalised_features\n",
        "\n",
        "def split_df_by_event(df_normalised_features, TOTAL_EVENTS):\n",
        "  # Dataframes split by event \n",
        "  df_event_list = []\n",
        "  for i in range(TOTAL_EVENTS):\n",
        "    df_event = df_normalised_features[df_normalised_features['event_list']==i]\n",
        "    df_event_list.append(df_event)\n",
        "\n",
        "  # A list of number of stable particles per event \n",
        "  length_of_each_event = [len(df_event_list[i]) for i in range(len(df_event_list))]\n",
        "  return df_event_list, length_of_each_event\n",
        "\n",
        "def create_source_target_for_COO(df_event_list):\n",
        "  # Add two columns of source, target over all dataframes in df_event_list to make it compatible with pygn Data Object.\n",
        "  df_event_source_target_list = []\n",
        "  for i in range(len(df_event_list)):\n",
        "    df_event_list[i]['source'] = None\n",
        "    df_event_list[i]['target'] = None\n",
        "    df_event_source_target_list.append(df_event_list[i])\n",
        "  return df_event_source_target_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGWiu1jwaDGU"
      },
      "source": [
        "#Generate Data.x and Data.y for pytorch geometric "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKRkJ576tz4R"
      },
      "source": [
        "# VARIABLE SIZE GRAPHS\n",
        "def generate_graph_data_x_y_for_already_normalised_features(df_event_processed_list_cleaned, TOTAL_EVENTS):\n",
        "  # Generating data.x and data.y for pytorch geomteric \n",
        "  graph_data_x_list = []\n",
        "  graph_data_y_list = []\n",
        "  for i in range(TOTAL_EVENTS):\n",
        "    df_graph = df_event_processed_list_cleaned[i]\n",
        "    # Extract node features and labels from cleaned processed fixed size event list and convert to numpy \n",
        "    data_x = df_graph[['pid', 'pos_r', 'pos_theta', 'pos_phi', 'pos_t', 'mom_p', 'mom_theta', 'mom_phi', 'mom_mass']].to_numpy()\n",
        "    data_y = df_graph[['label']].to_numpy()\n",
        "\n",
        "    # Convert numpy objects into tensors for data loaders \n",
        "    graph_data_x_list.append(torch.Tensor(data_x))\n",
        "    graph_data_y_list.append(torch.Tensor(data_y))\n",
        "  return graph_data_x_list, graph_data_y_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4bZZg6oD_Ua"
      },
      "source": [
        "# Create graph nodes and labels "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKW5T1fXt3aB"
      },
      "source": [
        "def create_graph_nodes_and_labels_for_variable_graphs(path_to_file, TOTAL_EVENTS):\n",
        "  df = load_df(path_to_file) # You can specify path to file here \n",
        "  TOTAL_EVENTS, _ = set_constants(TOTAL_EVENTS) # you can pass the constants here \n",
        "  df = create_labels(df) \n",
        "  df_normalised_features = normalise_x_features(df) # Don't call this if you are normalising features when creating graph dataset for FIXED GRAPHS  \n",
        "  df_event_list, length_of_each_event = split_df_by_event(df_normalised_features, TOTAL_EVENTS)\n",
        "  df_event_source_target_list = create_source_target_for_COO(df_event_list) \n",
        "  df_event_processed_list_cleaned = df_event_source_target_list\n",
        "  graph_data_x_list, graph_data_y_list = generate_graph_data_x_y_for_already_normalised_features(df_event_processed_list_cleaned, TOTAL_EVENTS)\n",
        "  return df, df_normalised_features, df_event_list, length_of_each_event, df_event_processed_list_cleaned, graph_data_x_list, graph_data_y_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHqYChKxURgj"
      },
      "source": [
        "# KNN \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iiupWWVUQ9W"
      },
      "source": [
        "def define_knn(num_neighbours=8):\n",
        "  knn = NearestNeighbors(n_neighbors=num_neighbours, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\n",
        "  return knn\n",
        "\n",
        "def generate_X_list_knn(knn, df_event_list, TOTAL_EVENTS, num_neighbours=8):\n",
        "  X_list_knn = [] \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    X = df_event_list[event_id]\n",
        "    X = X.drop(columns=['source', 'target'])\n",
        "    knn.fit(X)\n",
        "    neighbour = knn.kneighbors(X, n_neighbors=num_neighbours, return_distance=False)\n",
        "    target = neighbour\n",
        "    source = np.zeros((neighbour.shape))\n",
        "    X['source'] = None\n",
        "    X['target'] = None\n",
        "    for i in range(len(X)):\n",
        "      X['source'].iloc[i] = np.ones((neighbour.shape)) * i\n",
        "      X['target'].iloc[i] = target[i]\n",
        "    X_list_knn.append(X)\n",
        "  return X_list_knn\n",
        "\n",
        "def convert_COO_for_knn_events(df, num_neighbours=8):\n",
        "  source_list = [] \n",
        "  for i in range(len(df)):\n",
        "    for _ in range(num_neighbours):\n",
        "      source_list.append(i)\n",
        "  target_list = list(itertools.chain.from_iterable(df['target'].to_numpy()))\n",
        "  edge_index= torch.tensor([source_list, target_list], dtype=torch.long)\n",
        "  return edge_index\n",
        "\n",
        "def create_COO_format_data_knn_list(X_list_knn, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, num_neighbours=8):\n",
        "  # Data Represented as edges with K-nearest neighbours as 8 \n",
        "  # A list of graph data items to be passed on to the data loader \n",
        "  data_knn_list = []\n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    data_item = Data(x = graph_data_x_list[event_id], \n",
        "                    y = graph_data_y_list[event_id], \n",
        "                    edge_index = convert_COO_for_knn_events(X_list_knn[event_id], num_neighbours=8))\n",
        "    data_knn_list.append(data_item)\n",
        "  return data_knn_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsEQXxVriEez"
      },
      "source": [
        "def create_data_knn_list(df_event_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, num_neighbours=8):\n",
        "  # Create data knn list \n",
        "  knn = define_knn(num_neighbours=8)\n",
        "  X_list_knn = generate_X_list_knn(knn, df_event_list, TOTAL_EVENTS, num_neighbours=8)\n",
        "  data_knn_list = create_COO_format_data_knn_list(X_list_knn, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, num_neighbours=8)\n",
        "  return data_knn_list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjlYcvtHFJ6M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7UQ-C0vFK11"
      },
      "source": [
        "# Create KNN Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5r6qin7Fag9"
      },
      "source": [
        "def get_data_lists_for_variable_size_graphs(path_to_file, TOTAL_EVENTS, num_neighbours):\n",
        "  # VARIABLE SIZE GRAPHS\n",
        "  df, df_normalised_features, df_event_list, length_of_each_event, df_event_processed_list_cleaned, graph_data_x_list, graph_data_y_list = create_graph_nodes_and_labels_for_variable_graphs(path_to_file, TOTAL_EVENTS)\n",
        "  data_knn_list  = create_data_knn_list(df_event_list,graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, num_neighbours)\n",
        "  return data_knn_list, df_event_list, length_of_each_event, df_event_processed_list_cleaned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu5OH0MYRn6g"
      },
      "source": [
        "# calculate true mass and predicted mass for variable graphs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzaxkTiXGRTd"
      },
      "source": [
        "def get_true_mass_list(data_list, df_event_processed_list_cleaned, TOTAL_EVENTS):\n",
        "  total_true_mass_H_list = []\n",
        "  total_true_mass_Z_list = []\n",
        "  total_true_mass_O_list = []\n",
        "  column_index_for_mass = -1 # fixed \n",
        "  column_index_for_label = 0 # fixed for true labels\n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    true_mass_H_list = []\n",
        "    true_mass_Z_list = []\n",
        "    true_mass_O_list = []\n",
        "    event_length = len(df_event_processed_list_cleaned[event_id])\n",
        "    for node_id in range(event_length):\n",
        "      if data_list[event_id].y[node_id].numpy()[column_index_for_label] == 0:\n",
        "        true_mass_H_list.append(data_list[event_id].x[node_id].numpy()[column_index_for_mass])\n",
        "      elif data_list[event_id].y[node_id].numpy()[column_index_for_label] == 1:\n",
        "        true_mass_Z_list.append(data_list[event_id].x[node_id].numpy()[column_index_for_mass])\n",
        "      elif data_list[event_id].y[node_id].numpy()[column_index_for_label] == 2:\n",
        "        true_mass_O_list.append(data_list[event_id].x[node_id].numpy()[column_index_for_mass])\n",
        "    total_true_mass_H_list.append(true_mass_H_list)\n",
        "    total_true_mass_Z_list.append(true_mass_Z_list)\n",
        "    total_true_mass_O_list.append(true_mass_O_list)\n",
        "  return total_true_mass_H_list, total_true_mass_Z_list, total_true_mass_O_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6mUc4ngmqdv"
      },
      "source": [
        "def calculate_predicted_mass(data_list, df_event_processed_list_cleaned, pred_list, TOTAL_EVENTS):\n",
        "  total_pred_mass_H_list = []\n",
        "  total_pred_mass_Z_list = []\n",
        "  total_pred_mass_O_list = []\n",
        "  column_index_for_mass = -1 # fixed \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    pred_mass_H_list = []\n",
        "    pred_mass_Z_list = []\n",
        "    pred_mass_O_list = []\n",
        "    event_length = len(df_event_processed_list_cleaned[event_id])\n",
        "    for node_id in range(event_length):\n",
        "      pred = pred_list[event_id][node_id].numpy()\n",
        "      if pred == 0:\n",
        "        pred_mass_H_list.append(data_list[event_id].x[node_id].numpy()[column_index_for_mass])\n",
        "      elif pred == 1:\n",
        "        pred_mass_Z_list.append(data_list[event_id].x[node_id].numpy()[column_index_for_mass])\n",
        "      elif pred == 2:\n",
        "        pred_mass_O_list.append(data_list[event_id].x[node_id].numpy()[column_index_for_mass])\n",
        "    total_pred_mass_H_list.append(pred_mass_H_list)\n",
        "    total_pred_mass_Z_list.append(pred_mass_Z_list)\n",
        "    total_pred_mass_O_list.append(pred_mass_O_list)\n",
        "  return total_pred_mass_H_list, total_pred_mass_Z_list, total_pred_mass_O_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzzmg0KES7aq"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpJNEBh5Z5rb"
      },
      "source": [
        "# For Variable Graph Size \n",
        "def set_model_constants_variable():\n",
        "  dataset_num_features = 9 \n",
        "  dataset_num_classes = 3\n",
        "  return dataset_num_features, dataset_num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHNPHdAKSN0b"
      },
      "source": [
        "# Define the models\n",
        "class GCNX_relu(torch.nn.Module):\n",
        "    def __init__(self, dataset_num_features, hidden_channels, dataset_num_classes, dropout=0.5, num_layers=2):\n",
        "        super(GCNX_relu, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset_num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, dataset_num_classes )\n",
        "        self.convx = torch.nn.ModuleList([GCNConv(hidden_channels, hidden_channels) for _ in range(num_layers-2)])\n",
        "        self.dropout_p = dropout\n",
        "    \n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        for iter_layer in self.convx:\n",
        "            x = F.dropout(x, p= self.dropout_p, training=self.training)\n",
        "            x = F.relu(iter_layer(x, edge_index))\n",
        "        x = F.dropout(x, p= self.dropout_p, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# Define the models\n",
        "class GCNX_elu(torch.nn.Module):\n",
        "    def __init__(self, dataset_num_features, hidden_channels, dataset_num_classes, dropout=0.5, num_layers=2):\n",
        "        super(GCNX_elu, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset_num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, dataset_num_classes )\n",
        "        self.convx = torch.nn.ModuleList([GCNConv(hidden_channels, hidden_channels) for _ in range(num_layers-2)])\n",
        "        self.dropout_p = dropout\n",
        "    \n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        for iter_layer in self.convx:\n",
        "            x = F.dropout(x, p= self.dropout_p, training=self.training)\n",
        "            x = F.elu(iter_layer(x, edge_index))\n",
        "        x = F.dropout(x, p= self.dropout_p, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Define the models\n",
        "class GCNX_selu(torch.nn.Module):\n",
        "    def __init__(self, dataset_num_features, hidden_channels, dataset_num_classes, dropout=0.5, num_layers=2):\n",
        "        super(GCNX_selu, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset_num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, dataset_num_classes )\n",
        "        self.convx = torch.nn.ModuleList([GCNConv(hidden_channels, hidden_channels) for _ in range(num_layers-2)])\n",
        "        self.dropout_p = dropout\n",
        "    \n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.selu(self.conv1(x, edge_index))\n",
        "        for iter_layer in self.convx:\n",
        "            x = F.dropout(x, p= self.dropout_p, training=self.training)\n",
        "            x = F.selu(iter_layer(x, edge_index))\n",
        "        x = F.dropout(x, p= self.dropout_p, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Define the models\n",
        "class GCNX_gelu(torch.nn.Module):\n",
        "    def __init__(self, dataset_num_features, hidden_channels, dataset_num_classes, dropout=0.5, num_layers=2):\n",
        "        super(GCNX_gelu, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset_num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, dataset_num_classes )\n",
        "        self.convx = torch.nn.ModuleList([GCNConv(hidden_channels, hidden_channels) for _ in range(num_layers-2)])\n",
        "        self.dropout_p = dropout\n",
        "    \n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.gelu(self.conv1(x, edge_index))\n",
        "        for iter_layer in self.convx:\n",
        "            x = F.dropout(x, p= self.dropout_p, training=self.training)\n",
        "            x = F.gelu(iter_layer(x, edge_index))\n",
        "        x = F.dropout(x, p= self.dropout_p, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Define the models\n",
        "class GCNX_leakyrelu(torch.nn.Module):\n",
        "    def __init__(self, dataset_num_features, hidden_channels, dataset_num_classes, dropout=0.5, num_layers=2):\n",
        "        super(GCNX_leakyrelu, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset_num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, dataset_num_classes )\n",
        "        self.convx = torch.nn.ModuleList([GCNConv(hidden_channels, hidden_channels) for _ in range(num_layers-2)])\n",
        "        self.dropout_p = dropout\n",
        "    \n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
        "        for iter_layer in self.convx:\n",
        "            x = F.dropout(x, p= self.dropout_p, training=self.training)\n",
        "            x = F.leaky_relu(iter_layer(x, edge_index))\n",
        "        x = F.dropout(x, p= self.dropout_p, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# Define the models\n",
        "class GCNX_tanh(torch.nn.Module):\n",
        "    def __init__(self, dataset_num_features, hidden_channels, dataset_num_classes, dropout=0.5, num_layers=2):\n",
        "        super(GCNX_tanh, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset_num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, dataset_num_classes )\n",
        "        self.convx = torch.nn.ModuleList([GCNConv(hidden_channels, hidden_channels) for _ in range(num_layers-2)])\n",
        "        self.dropout_p = dropout\n",
        "    \n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.tanh(self.conv1(x, edge_index))\n",
        "        for iter_layer in self.convx:\n",
        "            x = F.dropout(x, p= self.dropout_p, training=self.training)\n",
        "            x = F.tanh(iter_layer(x, edge_index))\n",
        "        x = F.dropout(x, p= self.dropout_p, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3o_Eum1yPVO"
      },
      "source": [
        "# Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBErN1WaSQL0"
      },
      "source": [
        "def create_data_loaders(data_list, train_split=0.6):\n",
        "  # get length of the list\n",
        "  lenth_of_list = len(data_list)\n",
        "  train_split = 0.6\n",
        "  valid_split = 0.2\n",
        "  test_split = 0.2\n",
        "\n",
        "  # create indices for loaders \n",
        "  train_end_index = int(lenth_of_list * train_split)\n",
        "  validate_start_index = train_end_index\n",
        "  validate_end_index = validate_start_index + int(lenth_of_list * valid_split)\n",
        "  test_start_index = validate_end_index\n",
        "\n",
        "  train_loader = DataLoader(data_list[:train_end_index], batch_size=1, shuffle=False)\n",
        "  validation_loader = DataLoader(data_list[validate_start_index:validate_end_index], batch_size=1, shuffle=False)\n",
        "  test_loader = DataLoader(data_list[test_start_index:], batch_size=1, shuffle=False)\n",
        "  \n",
        "  return train_loader, validation_loader, test_loader, train_end_index, validate_start_index, validate_end_index, test_start_index "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Y8qbt-yRRt"
      },
      "source": [
        "# Train, Validate, Test, Cal Accuracy and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN3U9D1FSZKq"
      },
      "source": [
        "def train(model, optimizer, loader):\n",
        "  model.train()\n",
        "  pred_list, y_list, loss_list, y_list_numpy, pred_list_numpy = [], [],[], [], []\n",
        "  for batch in loader:\n",
        "    x, edges = batch.x, batch.edge_index\n",
        "    y = batch.y\n",
        "    y_list.append(y.squeeze(1).type(torch.LongTensor))\n",
        "    y_list_numpy.append(y.squeeze(1).type(torch.LongTensor).numpy())\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x, edges)\n",
        "    loss = criterion(out, y.squeeze(1).type(torch.LongTensor))\n",
        "    loss_list.append(loss)\n",
        "    pred = out.argmax(dim=1)\n",
        "    pred_list.append(pred)\n",
        "    pred_list_numpy.append(pred.numpy()) \n",
        "    loss.backward()\n",
        "    optimizer.step()  \n",
        "  return loss_list, pred_list, y_list, y_list_numpy,  pred_list_numpy\n",
        "\n",
        "def validate(model, optimizer, loader):\n",
        "  model.train()\n",
        "  pred_list, y_list, loss_list, y_list_numpy, pred_list_numpy = [], [],[], [], []\n",
        "  for batch in loader:\n",
        "    x, edges = batch.x, batch.edge_index\n",
        "    y = batch.y\n",
        "    y_list.append(y.squeeze(1).type(torch.LongTensor))\n",
        "    y_list_numpy.append(y.squeeze(1).type(torch.LongTensor).numpy())\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x, edges)\n",
        "    loss = criterion(out, y.squeeze(1).type(torch.LongTensor))\n",
        "    loss_list.append(loss)\n",
        "    pred = out.argmax(dim=1)\n",
        "    # convert tensor to numpy\n",
        "    pred_list.append(pred) \n",
        "    pred_list_numpy.append(pred.numpy()) \n",
        "    loss.backward()\n",
        "    optimizer.step()  \n",
        "  return loss_list, pred_list, y_list, y_list_numpy, pred_list_numpy\n",
        "\n",
        "def accuracy(pred_list, y_list):\n",
        "  accuracy_list = []\n",
        "  for event_id in range(len(pred_list)): # Because this works on the length of pred_list this logically surpasses the length mismatch issue\n",
        "    length_of_each_event = len(pred_list[event_id])\n",
        "    num_correct_list = []\n",
        "    for node_id in range(length_of_each_event):\n",
        "      p = pred_list[event_id][node_id].numpy()\n",
        "      y = y_list[event_id][node_id].numpy()\n",
        "      num_correct = p == y \n",
        "      num_correct_list.append(num_correct)\n",
        "    acc = sum(num_correct_list)/length_of_each_event\n",
        "    accuracy_list.append(acc)\n",
        "  return accuracy_list\n",
        "\n",
        "def train_on_n_epochs(n_epochs, model, optimizer, loader):\n",
        "  # running for multiple epochs\n",
        "  total_train_acc_list = []\n",
        "  total_train_loss_list, total_train_pred_list, total_train_y_list, total_train_y_list_numpy, total_train_pred_list_numpy = [], [], [], [], []\n",
        "  time_per_epoch_list = []\n",
        "  for _ in range(n_epochs):\n",
        "    train_acc_list = []\n",
        "    starttime = timeit.default_timer()\n",
        "    train_loss_list, train_pred_list, train_y_list, train_y_list_numpy, train_pred_list_numpy = train(model, optimizer, loader)\n",
        "    time_per_epoch_list.append(timeit.default_timer() - starttime) \n",
        "    train_acc_list = accuracy(train_pred_list, train_y_list)\n",
        "    total_train_acc_list.append(train_acc_list)\n",
        "    total_train_loss_list.append(train_loss_list)\n",
        "    total_train_y_list.append(train_y_list)\n",
        "    total_train_pred_list.append(train_pred_list)\n",
        "    total_train_y_list_numpy.append(train_y_list_numpy)\n",
        "    total_train_pred_list_numpy.append(train_pred_list_numpy)\n",
        "  return total_train_acc_list, total_train_loss_list, total_train_pred_list, total_train_y_list, total_train_y_list_numpy, total_train_pred_list_numpy, time_per_epoch_list\n",
        "\n",
        "def test(model, loader):\n",
        "  model.eval()\n",
        "  pred_list, y_list = [], []\n",
        "  y_list_numpy = []\n",
        "  for batch in loader:\n",
        "    x, edges = batch.x, batch.edge_index\n",
        "    y = batch.y\n",
        "    y_list.append(y.squeeze(1).type(torch.LongTensor))\n",
        "    y_list_numpy.append(y.squeeze(1).type(torch.LongTensor).numpy())\n",
        "    out = model(x, edges)\n",
        "    pred = out.argmax(dim=1)\n",
        "    pred_list.append(pred)    \n",
        "  return pred_list, y_list, y_list_numpy\n",
        "\n",
        "def evaluate_model_performace(model, loader):\n",
        "  test_pred_list, test_y_list, test_y_list_numpy = test(model, loader)\n",
        "  test_acc_list = []\n",
        "  x = accuracy(test_pred_list, test_y_list)\n",
        "  test_acc_list.append(x)\n",
        "  return test_acc_list, test_pred_list, test_y_list, test_y_list_numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txIyGPwZyW5v"
      },
      "source": [
        "# Init Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5l3ayOPWZEt"
      },
      "source": [
        "# Inits model and Adam optimizer \n",
        "# bool variable, int hidden_channels, float dropout, \n",
        "# int num_layers, string non_linearity, float learning_rate, float weight_decay\n",
        "\n",
        "# you can add an if else condition based on the type of optimiser passed as a string\n",
        "\n",
        "def init_GCNX_model(variable=True, non_linearity='relu', optimizer='adam', hidden_channels=16, dropout=0.5, num_layers=2,learning_rate=0.01, weight_decay=0.0005):\n",
        "  if variable == True:\n",
        "    dataset_num_features, dataset_num_classes = set_model_constants_variable()\n",
        "  elif variable == False:\n",
        "    dataset_num_features, dataset_num_classes = set_model_constants()\n",
        "  \n",
        "  if non_linearity == 'relu':\n",
        "    model = GCNX_relu(dataset_num_features, hidden_channels, dataset_num_classes, dropout, num_layers)\n",
        "  elif non_linearity == 'elu':\n",
        "    model = GCNX_elu(dataset_num_features, hidden_channels, dataset_num_classes, dropout, num_layers)\n",
        "  elif non_linearity == 'selu':\n",
        "    model = GCNX_selu(dataset_num_features, hidden_channels, dataset_num_classes, dropout, num_layers)\n",
        "  elif non_linearity == 'gelu':\n",
        "    model = GCNX_gelu(dataset_num_features, hidden_channels, dataset_num_classes, dropout, num_layers)\n",
        "  elif non_linearity == 'leakyrelu':\n",
        "    model = GCNX_leakyrelu(dataset_num_features, hidden_channels, dataset_num_classes, dropout, num_layers)\n",
        "  elif non_linearity == 'tanh':\n",
        "    model = GCNX_tanh(dataset_num_features, hidden_channels, dataset_num_classes, dropout, num_layers)\n",
        "\n",
        "  if optimizer == 'adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate, weight_decay=weight_decay)\n",
        "  elif optimizer == 'sgd':\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate, weight_decay=weight_decay)\n",
        "  elif optimizer == 'rmsprop':\n",
        "    optimizer = torch.optim.RMSprop(model.parameters(),lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  return dataset_num_features, dataset_num_classes, model, optimizer, criterion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMPyo1-pyI1F"
      },
      "source": [
        "# Experiment  KNN to save values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuesCEF6znBL"
      },
      "source": [
        "def save_to_experiment_path_as_dataframe(saved_list, path, file_name, dtype):\n",
        "  df = pd.DataFrame(data=saved_list, dtype=dtype)\n",
        "  df.to_csv(path + file_name)\n",
        "  return \"Saved as \" + path + file_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckbp74ymofqX"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c19HXeBaSfBK"
      },
      "source": [
        "def experiment_knn(data_list, df_event_list, model_knn, optimizer_knn, n_epochs = 1000, path_to_experiment = '/content/'):\n",
        "    # Data loaders for KNN (60:20:20)\n",
        "    train_loader, validation_loader, test_loader,  train_end_index, validate_start_index, validate_end_index, test_start_index = create_data_loaders(data_list)\n",
        "\n",
        "    # Calculate True Mass of each particle for each train/valid/test \n",
        "    train = df_event_list[:train_end_index]\n",
        "    valid = df_event_list[validate_start_index:validate_end_index]\n",
        "    test = df_event_list[test_start_index:]\n",
        "    # Split the data lists of graph events by (60:20:20)\n",
        "    data_list_train = data_list[:train_end_index]\n",
        "    data_list_valid = data_list[validate_start_index:validate_end_index]\n",
        "    data_list_test = data_list[test_start_index:]\n",
        "    # Call method over the 3 lists \n",
        "    total_true_mass_H_list_train, total_true_mass_Z_list_train, total_true_mass_O_list_train = get_true_mass_list(data_list_train, train, len(train))\n",
        "    total_true_mass_H_list_valid, total_true_mass_Z_list_valid, total_true_mass_O_list_valid = get_true_mass_list(data_list_valid, valid, len(valid))\n",
        "    total_true_mass_H_list_test, total_true_mass_Z_list_test, total_true_mass_O_list_test = get_true_mass_list(data_list_test, test, len(test))\n",
        "\n",
        "    # Training on KNN Dataset\n",
        "    total_train_acc_list, total_train_loss_list, total_train_pred_list, total_train_y_list, total_train_y_list_numpy, total_train_pred_list_numpy, train_time_per_epoch_list = train_on_n_epochs(n_epochs, model_knn, optimizer_knn, train_loader)\n",
        "    total_valid_acc_list, total_valid_loss_list, total_valid_pred_list, total_valid_y_list, total_valid_y_list_numpy, total_valid_pred_list_numpy, valid_time_per_epoch_list = train_on_n_epochs(n_epochs, model_knn, optimizer_knn, validation_loader)\n",
        "    mean_train_epoch_time = sum(train_time_per_epoch_list)/len(train_time_per_epoch_list)\n",
        "    mean_valid_epoch_time = sum(valid_time_per_epoch_list)/len(valid_time_per_epoch_list)\n",
        "\n",
        "    # Testing on KNN Data set \n",
        "    test_acc_list, test_pred_list, test_y_list, test_y_list_numpy = evaluate_model_performace(model_knn, test_loader)\n",
        "    \n",
        "    # Calculate the set of predicted mass for each particle\n",
        "    total_pred_mass_H_list_train, total_pred_mass_Z_list_train, total_pred_mass_O_list_train = calculate_predicted_mass(data_list_train, train, total_train_pred_list[-1], len(train))\n",
        "    total_pred_mass_H_list_valid, total_pred_mass_Z_list_valid, total_pred_mass_O_list_valid = calculate_predicted_mass(data_list_valid, valid, total_valid_pred_list[-1], len(valid))\n",
        "    total_pred_mass_H_list_test, total_pred_mass_Z_list_test, total_pred_mass_O_list_test = calculate_predicted_mass(data_list_test, test, test_pred_list, len(test))\n",
        "\n",
        "    # Results\n",
        "    train_acc_knn = ['knn_train', np.mean(total_train_acc_list[-1]),np.std(total_train_acc_list[-1])]\n",
        "    valid_acc_knn = ['knn_valid', np.mean(total_valid_acc_list[-1]),np.std(total_valid_acc_list[-1])]\n",
        "    test_acc_knn = ['knn_test', np.mean(test_acc_list), np.std(test_acc_list) ]\n",
        "\n",
        "    df_result_knn = pd.DataFrame([train_acc_knn, valid_acc_knn, test_acc_knn ], columns=['type','mean', 'stdev'])\n",
        "\n",
        "    # save to drive \n",
        "    df_result_knn.to_csv(path_to_experiment  + 'df_result_knn.csv' )\n",
        "    print(df_result_knn)\n",
        "\n",
        "    # fix file names \n",
        "    file_name1 = 'df_knn_train_acc.csv'\n",
        "    file_name2 = 'df_knn_test_acc.csv'\n",
        "    file_name3 = 'df_knn_train_loss.csv'\n",
        "    file_name4 = 'df_knn_test_pred.csv'\n",
        "    file_name5 = 'df_knn_test_y.csv'\n",
        "    file_name6 = 'df_knn_train_pred.csv'\n",
        "    file_name7 = 'df_knn_train_y.csv'\n",
        "    file_name8 = 'df_knn_train_time_per_epoch_list.csv'\n",
        "    file_name9 = 'df_knn_valid_time_per_epoch_list.csv'\n",
        "    file_name10 = 'df_knn_valid_acc.csv'\n",
        "    file_name11 = 'df_knn_valid_loss.csv'\n",
        "    file_name12 = 'df_knn_valid_pred.csv'\n",
        "    file_name13 = 'df_knn_valid_y.csv'\n",
        "    file_name14 = 'df_knn_test_y_numpy.csv'\n",
        "    file_name15 = 'df_knn_train_y_numpy.csv'\n",
        "    file_name16 = 'df_knn_valid_y_numpy.csv'\n",
        "    file_name17 = \"df_knn_true_mass_H_list_train\"\n",
        "    file_name18 = \"df_knn_true_mass_Z_list_train\"\n",
        "    file_name19 = \"df_knn_true_mass_O_list_train\"\n",
        "    file_name20 = \"df_knn_true_mass_H_list_valid\"\n",
        "    file_name21 = \"df_knn_true_mass_Z_list_valid\"\n",
        "    file_name22 = \"df_knn_true_mass_O_list_valid\"\n",
        "    file_name23 = \"df_knn_true_mass_H_list_test\"\n",
        "    file_name24 = \"df_knn_true_mass_Z_list_test\"\n",
        "    file_name25 = \"df_knn_true_mass_O_list_test\"\n",
        "    file_name26 = \"df_knn_pred_mass_H_list_train\"\n",
        "    file_name27 = \"df_knn_pred_mass_Z_list_train\"\n",
        "    file_name28 = \"df_knn_pred_mass_O_list_train\"\n",
        "    file_name29 = \"df_knn_pred_mass_H_list_valid\"\n",
        "    file_name30 = \"df_knn_pred_mass_Z_list_valid\"\n",
        "    file_name31 = \"df_knn_pred_mass_O_list_valid\"\n",
        "    file_name32 = \"df_knn_pred_mass_H_list_test\"\n",
        "    file_name33 = \"df_knn_pred_mass_Z_list_test\"\n",
        "    file_name34 = \"df_knn_pred_mass_O_list_test\"\n",
        "    file_name35 = 'df_knn_train_pred_numpy.csv'\n",
        "    file_name36 = 'df_knn_valid_pred_numpy.csv'\n",
        "\n",
        "    # use path_to_experiment\n",
        "    save_to_experiment_path_as_dataframe(total_train_acc_list,  path_to_experiment, file_name1, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(test_acc_list, path_to_experiment, file_name2, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_train_loss_list, path_to_experiment, file_name3, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(test_pred_list, path_to_experiment, file_name4, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(test_y_list, path_to_experiment, file_name5, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(total_train_pred_list, path_to_experiment, file_name6, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(total_train_y_list, path_to_experiment, file_name7, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(train_time_per_epoch_list, path_to_experiment, file_name8, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(valid_time_per_epoch_list, path_to_experiment, file_name9, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_acc_list,  path_to_experiment, file_name10, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_loss_list, path_to_experiment, file_name11, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_pred_list, path_to_experiment, file_name12, dtype=object)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_y_list, path_to_experiment, file_name13, dtype=object)\n",
        "    # test y labels \n",
        "    save_to_experiment_path_as_dataframe(test_y_list_numpy, path_to_experiment, file_name14, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_train_y_list_numpy, path_to_experiment, file_name15, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_y_list_numpy, path_to_experiment, file_name16, dtype=float)\n",
        "    # mass true\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_H_list_train,  path_to_experiment, file_name17, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_Z_list_train,  path_to_experiment, file_name18, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_O_list_train,  path_to_experiment, file_name19, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_H_list_valid,  path_to_experiment, file_name20, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_Z_list_valid,  path_to_experiment, file_name21, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_O_list_valid,  path_to_experiment, file_name22, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_H_list_test,  path_to_experiment, file_name23, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_Z_list_test,  path_to_experiment, file_name24, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_true_mass_O_list_test,  path_to_experiment, file_name25, dtype=float)\n",
        "\n",
        "    # mass calculated\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_H_list_train,  path_to_experiment, file_name26, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_Z_list_train,  path_to_experiment, file_name27, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_O_list_train,  path_to_experiment, file_name28, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_H_list_valid,  path_to_experiment, file_name29, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_Z_list_valid,  path_to_experiment, file_name30, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_O_list_valid,  path_to_experiment, file_name31, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_H_list_test,  path_to_experiment, file_name32, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_Z_list_test,  path_to_experiment, file_name33, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_pred_mass_O_list_test,  path_to_experiment, file_name34, dtype=float)\n",
        "    # numpy pred labels \n",
        "    save_to_experiment_path_as_dataframe(total_train_pred_list_numpy, path_to_experiment, file_name35, dtype=float)\n",
        "    save_to_experiment_path_as_dataframe(total_valid_pred_list_numpy, path_to_experiment, file_name36, dtype=float)\n",
        "\n",
        "    # Save model \n",
        "    torch.save(model_knn.state_dict(), path_to_experiment + 'model_knn.pickle')\n",
        "    return mean_train_epoch_time, mean_valid_epoch_time, df_result_knn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3oFQxuz1ar8"
      },
      "source": [
        "# Create KNN Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzOyUz6JY6qq"
      },
      "source": [
        "# Create the data_knn_list_k_2\n",
        "path_to_file = '/content/drive/MyDrive/FCC_Experiments_2021/dataset/processed_csv_files/output_11_07_2021.csv'\n",
        "data_knn_list_variable, df_event_list_variable, length_of_each_event_variable, df_event_processed_list_cleaned_variable = get_data_lists_for_variable_size_graphs(path_to_file, TOTAL_EVENTS=10000, num_neighbours=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wobXyFf0IH9"
      },
      "source": [
        "# GCN 2 layer variable model on the KNN Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zjZBnkNzq6j"
      },
      "source": [
        "We will only **run the experiments on KNN dataset** as they have proved to be the most effective and accurate over other datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awTR6skq0UVi"
      },
      "source": [
        "The hyperparameters that will be used to test the performance of the model are as follows:\n",
        "1. K nearest neighbours\n",
        "2. Non-Linearity\n",
        "3. Hidden Channels\n",
        "4. Dropout\n",
        "5. Learning Rate\n",
        "6. Weight Decay \n",
        "7. Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6jfrYtVajoB"
      },
      "source": [
        "# SET PATHS and FILE NAMES\n",
        "path_to_experiment_variable_depth_2_gcn_dropout_1 = '/content/drive/MyDrive/FCC_Experiments_2021/model_gcn/variable/depth_2/hyperparameters/dropout_x/dropout_0.1/'\n",
        "\n",
        "path_to_experiment_variable_depth_2_gcn_dropout_2 = '/content/drive/MyDrive/FCC_Experiments_2021/model_gcn/variable/depth_2/hyperparameters/dropout_x/dropout_0.2/'\n",
        "\n",
        "path_to_experiment_variable_depth_2_gcn_dropout_3 = '/content/drive/MyDrive/FCC_Experiments_2021/model_gcn/variable/depth_2/hyperparameters/dropout_x/dropout_0.3/'\n",
        "\n",
        "path_to_experiment_variable_depth_2_gcn_dropout_4 = '/content/drive/MyDrive/FCC_Experiments_2021/model_gcn/variable/depth_2/hyperparameters/dropout_x/dropout_0.4/'\n",
        "\n",
        "path_to_experiment_variable_depth_2_gcn_dropout_5 = '/content/drive/MyDrive/FCC_Experiments_2021/model_gcn/variable/depth_2/hyperparameters/dropout_x/dropout_0.5/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgieFIej_WdI"
      },
      "source": [
        "# GCN - Dropout \n",
        "1. Dropout =0.1\n",
        "2. Dropout =0.2\n",
        "3. Dropout =0.3\n",
        "4. Dropout =0.4\n",
        "5. Dropout =0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU_jP3IZbDfd"
      },
      "source": [
        "# Dropout = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7dAuS4H_DQi",
        "outputId": "ad979071-4837-46a8-d812-ada2e6a1227b"
      },
      "source": [
        "dataset_num_features, dataset_num_classes, model_knn_variable_depth_2_gcn, optimizer_knn_variable_depth_2_gcn, criterion= init_GCNX_model(variable=True, non_linearity='relu', optimizer='adam', hidden_channels=16, dropout=0.1, num_layers=2,learning_rate=0.01, weight_decay=0.0005)\n",
        "path_to_experiment_variable_depth_2_gcn_dropout_1 = '/content/drive/MyDrive/FCC_Experiments_2021/model_gcn/variable/depth_2/hyperparameters/dropout_x/dropout_0.1/'\n",
        "train_epoch_knn_2, valid_epoch_knn_2, df_result_knn_2 = experiment_knn(data_knn_list_variable, df_event_processed_list_cleaned_variable, model_knn_variable_depth_2_gcn, optimizer_knn_variable_depth_2_gcn, n_epochs = 51, path_to_experiment = path_to_experiment_variable_depth_2_gcn_dropout_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        type      mean     stdev\n",
            "0  knn_train  0.997477  0.015262\n",
            "1  knn_valid  0.997704  0.012563\n",
            "2   knn_test  0.997852  0.006924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTiWEahobF_F"
      },
      "source": [
        "#Dropout = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUsCuOYP_jls",
        "outputId": "f2359b01-113c-4c84-aa23-ea52c29a5a16"
      },
      "source": [
        "dataset_num_features, dataset_num_classes, model_knn_variable_depth_2_gcn, optimizer_knn_variable_depth_2_gcn, criterion= init_GCNX_model(variable=True, non_linearity='relu', optimizer='adam', hidden_channels=16, dropout=0.2, num_layers=2,learning_rate=0.01, weight_decay=0.0005)\n",
        "path_to_experiment_variable_depth_2_gcn_dropout_2 = '/content/drive/MyDrive/FCC_Experiments_2021/model_gcn/variable/depth_2/hyperparameters/dropout_x/dropout_0.2/'\n",
        "train_epoch_knn_2, valid_epoch_knn_2, df_result_knn_2 = experiment_knn(data_knn_list_variable, df_event_processed_list_cleaned_variable, model_knn_variable_depth_2_gcn, optimizer_knn_variable_depth_2_gcn, n_epochs = 51, path_to_experiment = path_to_experiment_variable_depth_2_gcn_dropout_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        type      mean     stdev\n",
            "0  knn_train  0.997453  0.015351\n",
            "1  knn_valid  0.997809  0.012281\n",
            "2   knn_test  0.997987  0.006747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-TRxOmmbKo-"
      },
      "source": [
        "# Dropout = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v43Qxj4-_kbd",
        "outputId": "b0a60e78-b63d-4770-e484-5a8158ff2dd9"
      },
      "source": [
        "dataset_num_features, dataset_num_classes, model_knn_variable_depth_2_gcn, optimizer_knn_variable_depth_2_gcn, criterion= init_GCNX_model(variable=True, non_linearity='relu', optimizer='adam', hidden_channels=16, dropout=0.3, num_layers=2,learning_rate=0.01, weight_decay=0.0005)\n",
        "path_to_experiment_variable_depth_2_gcn_dropout_3 = '/content/drive/MyDrive/FCC_Experiments_2021/model_gcn/variable/depth_2/hyperparameters/dropout_x/dropout_0.3/'\n",
        "\n",
        "train_epoch_knn_2, valid_epoch_knn_2, df_result_knn_2 = experiment_knn(data_knn_list_variable, df_event_processed_list_cleaned_variable, model_knn_variable_depth_2_gcn, optimizer_knn_variable_depth_2_gcn, n_epochs = 51, path_to_experiment =path_to_experiment_variable_depth_2_gcn_dropout_3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        type      mean     stdev\n",
            "0  knn_train  0.997558  0.015224\n",
            "1  knn_valid  0.997726  0.012410\n",
            "2   knn_test  0.998032  0.006670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuJy65lWbR81"
      },
      "source": [
        "# Dropout = 0.4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1XogynX_lMr",
        "outputId": "d45d5caf-c317-4caf-c555-209654f1232b"
      },
      "source": [
        "dataset_num_features, dataset_num_classes, model_knn_variable_depth_2_gcn, optimizer_knn_variable_depth_2_gcn, criterion= init_GCNX_model(variable=True, non_linearity='relu', optimizer='adam', hidden_channels=16, dropout=0.4, num_layers=2,learning_rate=0.01, weight_decay=0.0005)\n",
        "path_to_experiment_variable_depth_2_gcn_dropout_4 = '/content/drive/MyDrive/FCC_Experiments_2021/model_gcn/variable/depth_2/hyperparameters/dropout_x/dropout_0.4/'\n",
        "\n",
        "train_epoch_knn_2, valid_epoch_knn_2, df_result_knn_2 = experiment_knn(data_knn_list_variable, df_event_processed_list_cleaned_variable, model_knn_variable_depth_2_gcn, optimizer_knn_variable_depth_2_gcn, n_epochs = 51, path_to_experiment = path_to_experiment_variable_depth_2_gcn_dropout_4 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        type      mean     stdev\n",
            "0  knn_train  0.997507  0.015306\n",
            "1  knn_valid  0.997782  0.012378\n",
            "2   knn_test  0.998467  0.005931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpidRTLfbWzB"
      },
      "source": [
        "# Dropout = 0.5 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJI-llBn_mYP",
        "outputId": "b162f177-d0f4-41bf-bb4f-485ffeb8dc90"
      },
      "source": [
        "dataset_num_features, dataset_num_classes, model_knn_variable_depth_2_gcn, optimizer_knn_variable_depth_2_gcn, criterion= init_GCNX_model(variable=True, non_linearity='relu', optimizer='adam', hidden_channels=16, dropout=0.5, num_layers=2,learning_rate=0.01, weight_decay=0.0005)\n",
        "path_to_experiment_variable_depth_2_gcn_dropout_5 = '/content/drive/MyDrive/FCC_Experiments_2021/model_gcn/variable/depth_2/hyperparameters/dropout_x/dropout_0.5/'\n",
        "\n",
        "train_epoch_knn_2, valid_epoch_knn_2, df_result_knn_2 = experiment_knn(data_knn_list_variable, df_event_processed_list_cleaned_variable, model_knn_variable_depth_2_gcn, optimizer_knn_variable_depth_2_gcn, n_epochs = 51, path_to_experiment = path_to_experiment_variable_depth_2_gcn_dropout_5 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        type      mean     stdev\n",
            "0  knn_train  0.997351  0.016184\n",
            "1  knn_valid  0.997796  0.012294\n",
            "2   knn_test  0.998618  0.005674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVg4AnC0GqjP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}