{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fcc_experiment_8.x_model_performance_JK_4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNIdOYa6320uhvNRYe9ZzUw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akanksha-ahuja/fcc-final-notebooks/blob/main/fcc_experiment_8_x_model_performance_JK_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GzGCMXLR7Vh"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import f1_score, matthews_corrcoef\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89ji-3-gVcu4",
        "outputId": "358f5c72-9997-4c35-b97d-b636f4b427d6"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csGsEK_kWRtk"
      },
      "source": [
        "## Performance of JK 4 layer variable size "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD03jgqjWVm5"
      },
      "source": [
        "path_to_experiment_variable_depth_4_jk_radius = '/content/drive/MyDrive/FCC_Experiments_2021/model_jk/variable/depth_4/aggr_lstm/radius/'\n",
        "path_to_experiment_variable_depth_4_jk_knn = '/content/drive/MyDrive/FCC_Experiments_2021/model_jk/variable/depth_4/aggr_lstm/knn/'\n",
        "path_to_experiment_variable_depth_4_jk_label = '/content/drive/MyDrive/FCC_Experiments_2021/model_jk/variable/depth_4/aggr_lstm/label/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrWADUA_Z_a9"
      },
      "source": [
        "## Methods for performance metrics \n",
        "1. Event Accuracy \n",
        "2. Performance Metrics - F1 score, Matthew CorrCoef"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLHztCy-m5PD"
      },
      "source": [
        "## Event Accuracy: % of events classified with 100% accuracy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWURMC0bccPg"
      },
      "source": [
        "# Caclulates the percentage of 100% correctly classified events\n",
        "def calculate_event_accuracy(df):\n",
        "    NUM_EVENTS = len(df.to_numpy().flatten())\n",
        "    unique, counts = np.unique(df.to_numpy(), return_counts=True)\n",
        "    acc_dict = dict(zip(unique, counts))\n",
        "    # print(acc_dict)\n",
        "    try: \n",
        "      num_events_classified_with_100_percent_acc = acc_dict[1.0]\n",
        "      # print(num_events_classified_with_100_percent_acc)\n",
        "      # print(NUM_EVENTS)\n",
        "      event_accuracy = num_events_classified_with_100_percent_acc/NUM_EVENTS\n",
        "      # print(event_accuracy)\n",
        "      return event_accuracy\n",
        "    except:\n",
        "      print(\"There are no events with 100% percent accuracy\")\n",
        "      return 0 \n",
        "\n",
        "# Saves the event accuracy over 3 datasets\n",
        "def save_event_accuracy(event_accuracy_train, event_accuracy_valid, event_accuracy_test, path_to_experiment, file_name='df_event_accuracy'):\n",
        "  event_accuracy  = [ event_accuracy_train, event_accuracy_valid, event_accuracy_test]\n",
        "  df = pd.DataFrame([event_accuracy], columns=['event_accuracy_train', 'event_accuracy_valid', 'event_accuracy_test' ])\n",
        "  df.to_csv(path_to_experiment + file_name, index=False)\n",
        "  print(df)\n",
        "  return df\n",
        "\n",
        "# Calculates and stores the final df of event accuracy over train-valid-test\n",
        "def store_event_accuracy(df_train_acc, df_valid_acc, df_test_acc, path_to_experiment):\n",
        "  event_accuracy_train = calculate_event_accuracy(df_train_acc)\n",
        "  event_accuracy_test = calculate_event_accuracy(df_valid_acc)\n",
        "  event_accuracy_valid = calculate_event_accuracy(df_test_acc)\n",
        "  df_event_accuracy = save_event_accuracy(event_accuracy_train, event_accuracy_valid, event_accuracy_test, path_to_experiment)\n",
        "  return df_event_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXuqtHe3myYI"
      },
      "source": [
        "## Methods of Performance metrics of node classification for each event"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StqmpNT5eZ8J"
      },
      "source": [
        "def calculate_f1_score(df_y, df_pred):\n",
        "  f1_score_macro = f1_score(df_y, df_pred, average='macro')\n",
        "  f1_score_micro = f1_score(df_y, df_pred, average='micro')\n",
        "  f1_score_weighted = f1_score(df_y, df_pred, average='weighted')\n",
        "  return f1_score_macro, f1_score_micro, f1_score_weighted\n",
        "\n",
        "def calculate_matthews_correlation_coefficient(df_y, df_pred):\n",
        "  matthews_correlation_coefficient = matthews_corrcoef(df_y, df_pred)\n",
        "  return matthews_correlation_coefficient\n",
        "\n",
        "# Saves both the mean and the list of performance metrics\n",
        "def save_event_performance_metrics(f1_score_macro, f1_score_micro, f1_score_weighted, matthews_correlation_coefficient, path_to_experiment, file_name='df_performance_metrics'):\n",
        "  performance_metrics  = [f1_score_macro, f1_score_micro, f1_score_weighted, matthews_correlation_coefficient]\n",
        "  df = pd.DataFrame([performance_metrics], columns=['f1_score_macro', 'f1_score_micro', 'f1_score_weighted', 'matthews_corrcoef'])\n",
        "  df.to_csv(path_to_experiment + file_name, index=False)\n",
        "  print(df)\n",
        "  return df\n",
        "\n",
        "# Calculates the score of single event and then appends to a list\n",
        "def overall_performance_metrics(df_y, df_pred):\n",
        "  # Calculates the scores for all events in the test dataset \n",
        "  f1_score_macro_list, f1_score_micro_list, f1_score_weighted_list  = [], [], []\n",
        "  matthews_correlation_coefficient_list = []\n",
        "  \n",
        "  for event_id in range(len(df_y)):\n",
        "    f1_score_macro, f1_score_micro, f1_score_weighted = calculate_f1_score(df_y=df_y.iloc[event_id].dropna(), df_pred=df_pred.iloc[event_id].dropna())\n",
        "    matthews_correlation_coefficient = calculate_matthews_correlation_coefficient(df_y=df_y.iloc[event_id].dropna(), df_pred=df_pred.iloc[event_id].dropna())\n",
        "    f1_score_macro_list.append(f1_score_macro)\n",
        "    f1_score_micro_list.append(f1_score_micro)\n",
        "    f1_score_weighted_list.append(f1_score_weighted)\n",
        "    matthews_correlation_coefficient_list.append(matthews_correlation_coefficient)\n",
        "\n",
        "  return f1_score_macro_list, f1_score_micro_list, f1_score_weighted_list, matthews_correlation_coefficient_list\n",
        "\n",
        "# Calculates the f1 score and matthew corrcoef score for all events \n",
        "def calculate_performance_metrics(df_y, df_pred, path_to_experiment):\n",
        "  f1_score_macro_list, f1_score_micro_list, f1_score_weighted_list, matthews_correlation_coefficient_list = overall_performance_metrics(df_y, df_pred)\n",
        "  df_performance_metrics = save_event_performance_metrics(f1_score_macro_list, f1_score_micro_list, f1_score_weighted_list, matthews_correlation_coefficient_list, path_to_experiment)\n",
        "  return df_performance_metrics\n",
        "\n",
        "\n",
        "# Calculates the mean of f1 score and matthew corrcoef score for all events \n",
        "def calculate_average_performance_metrics(df_performance_metrics, path_to_experiment):\n",
        "  avg_f1_score_macro = sum(df_performance_metrics['f1_score_macro'][0])/len(df_performance_metrics['f1_score_macro'][0])\n",
        "  avg_f1_score_micro = sum(df_performance_metrics['f1_score_micro'][0])/len(df_performance_metrics['f1_score_micro'][0])\n",
        "  avg_f1_score_weighted = sum(df_performance_metrics['f1_score_weighted'][0])/len(df_performance_metrics['f1_score_weighted'][0])\n",
        "  avg_matthews_correlation_coefficient = sum(df_performance_metrics['matthews_corrcoef'][0])/len(df_performance_metrics['matthews_corrcoef'][0])\n",
        "  df_average_performance_metrics = save_event_performance_metrics(avg_f1_score_macro,avg_f1_score_micro,avg_f1_score_weighted,avg_matthews_correlation_coefficient, path_to_experiment, file_name='df_average_performance_metrics')\n",
        "  return df_average_performance_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95C45lrVkwQN"
      },
      "source": [
        "**Compute the F1 score**, also known as balanced F-score or F-measure.\n",
        "\n",
        "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
        "\n",
        "```F1 = 2 * (precision * recall) / (precision + recall)```\n",
        "\n",
        "In the multi-class and multi-label case, this is the average of the F1 score of each class with weighting depending on the average parameter.\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nALIulHBkWD5"
      },
      "source": [
        "**The Matthews correlation coefficient** is used in machine learning as a measure of the quality of binary and multiclass classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient. Binary and multiclass labels are supported. \n",
        "\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrlRINa2NgT3"
      },
      "source": [
        "## Plot and Calculate average loss and average accuracies "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBGklXNdNlgR"
      },
      "source": [
        "# Calculates average accuracy over a range of events for each epoch\n",
        "def calculate_average_accuracy(df_acc):\n",
        "  average_acc = []\n",
        "  for event_id in range(len(df_acc)):\n",
        "    average_acc_per_event = sum(df_acc.iloc[event_id].to_numpy())/len(df_acc.iloc[event_id].to_numpy())\n",
        "    average_acc.append(average_acc_per_event)\n",
        "  return average_acc\n",
        "\n",
        "# Calculates average loss over a range of events for each epoch\n",
        "def calculate_average_loss(df_loss):\n",
        "  average_loss = []\n",
        "  for event_id in range(len(df_loss)):\n",
        "    average_loss_per_event = sum(df_loss.iloc[event_id].to_numpy())/len(df_loss.iloc[event_id].to_numpy())\n",
        "    average_loss.append(average_loss_per_event)\n",
        "  return average_loss\n",
        "\n",
        "# Saves final accuracy for train-valid-test\n",
        "def save_train_valid_test_accuracy_metrics(final_acc_train, final_acc_valid, final_test_acc, path_to_experiment, file_name='df_accuracy_metrics'):\n",
        "  performance_metrics  = [final_acc_train, final_acc_valid, final_test_acc]\n",
        "  df = pd.DataFrame([performance_metrics], columns=['final_acc_train', 'final_acc_valid', 'final_test_acc'])\n",
        "  df.to_csv(path_to_experiment + file_name, index=False)\n",
        "  print(df)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmsJgILA3Cqv"
      },
      "source": [
        "## Get metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZOFDn9q3GyP"
      },
      "source": [
        "def get_performance_metrics(df_y,  df_pred, df_train_acc,df_valid_acc, df_test_acc, path_to_experiment):\n",
        "  df_performance_metrics = calculate_performance_metrics(df_y, df_pred, path_to_experiment)\n",
        "  df_average_performance_metrics = calculate_average_performance_metrics(df_performance_metrics, path_to_experiment)\n",
        "  df_event_accuracy = store_event_accuracy(df_train_acc, df_valid_acc, df_test_acc, path_to_experiment)\n",
        "  return df_performance_metrics, df_average_performance_metrics, df_event_accuracy\n",
        "\n",
        "def get_accuracy_metrics(df_train_acc, df_valid_acc, df_test_acc, df_train_loss, df_valid_loss, path_to_experiment):\n",
        "  # List of average accuracy and losses over all events for train and valid datasets\n",
        "  average_acc_train = calculate_average_accuracy(df_train_acc)\n",
        "  average_acc_valid = calculate_average_accuracy(df_valid_acc)\n",
        "  average_loss_train = calculate_average_loss(df_train_loss)\n",
        "  average_loss_valid = calculate_average_loss(df_valid_loss)\n",
        "\n",
        "  # List of event-wise accuracy on test dataset\n",
        "  test_acc = df_test_acc.to_numpy().flatten()\n",
        "\n",
        "  # Final single number - averaged across all events \n",
        "  final_test_acc = sum(test_acc)/len(test_acc)\n",
        "  final_acc_train = sum(average_acc_train)/len(average_acc_train )\n",
        "  final_acc_valid = sum(average_acc_valid)/len(average_acc_valid )\n",
        "\n",
        "  # save the file for accuracy metrics\n",
        "  df_accuracy_metrics = save_train_valid_test_accuracy_metrics(final_acc_train, final_acc_valid, final_test_acc, path_to_experiment)\n",
        "\n",
        "  return  df_accuracy_metrics, average_acc_train,average_acc_valid, average_loss_train, average_loss_valid, test_acc, final_test_acc, final_acc_train \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4mwzVGx5gtZ"
      },
      "source": [
        "## Get model outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfdriIr-5ink"
      },
      "source": [
        "def get_model_outputs(path_to_experiment, df_type = 'df_knn_'):\n",
        "  # Get all files saved during model execution \n",
        "  df1 = pd.read_csv(path_to_experiment + df_type + \"pred_mass_H_list_test\").drop('Unnamed: 0', axis=1)\n",
        "  df2 = pd.read_csv(path_to_experiment + df_type  + \"pred_mass_H_list_train\").drop('Unnamed: 0', axis=1)\n",
        "  df3 = pd.read_csv(path_to_experiment + df_type + \"pred_mass_H_list_valid\").drop('Unnamed: 0', axis=1)\n",
        "  df4 = pd.read_csv(path_to_experiment + df_type + \"pred_mass_O_list_test\").drop('Unnamed: 0', axis=1)\n",
        "  df5 = pd.read_csv(path_to_experiment + df_type + \"pred_mass_O_list_train\").drop('Unnamed: 0', axis=1)\n",
        "  df6 = pd.read_csv(path_to_experiment + df_type + \"pred_mass_O_list_valid\").drop('Unnamed: 0', axis=1)\n",
        "  df7 = pd.read_csv(path_to_experiment + df_type + \"pred_mass_Z_list_test\").drop('Unnamed: 0', axis=1)\n",
        "  df8 = pd.read_csv(path_to_experiment + df_type + \"pred_mass_Z_list_train\").drop('Unnamed: 0', axis=1)\n",
        "  df9 = pd.read_csv(path_to_experiment + df_type + \"pred_mass_Z_list_valid\").drop('Unnamed: 0', axis=1)\n",
        "\n",
        "  df10 = pd.read_csv(path_to_experiment + df_type + \"test_acc.csv\").drop('Unnamed: 0', axis=1)\n",
        "  df11 = pd.read_csv(path_to_experiment + df_type + \"test_pred.csv\").drop('Unnamed: 0', axis=1)\n",
        "  df12 = pd.read_csv(path_to_experiment + df_type + \"test_y.csv\").drop('Unnamed: 0', axis=1)\n",
        "  df13 = pd.read_csv(path_to_experiment + df_type + \"test_y_numpy.csv\").drop('Unnamed: 0', axis=1)\n",
        "\n",
        "  df14 = pd.read_csv(path_to_experiment + df_type + \"train_acc.csv\").drop('Unnamed: 0', axis=1)\n",
        "  df15 = pd.read_csv(path_to_experiment + df_type + \"train_loss.csv\").drop('Unnamed: 0', axis=1)\n",
        "  df16 = pd.read_csv(path_to_experiment + df_type + \"train_pred.csv\").drop('Unnamed: 0', axis=1)\n",
        "  df17 = pd.read_csv(path_to_experiment + df_type + \"train_time_per_epoch_list.csv\").drop('Unnamed: 0', axis=1)\n",
        "  df18 = pd.read_csv(path_to_experiment + df_type + \"train_y.csv\").drop('Unnamed: 0', axis=1)\n",
        "  df19 = pd.read_csv(path_to_experiment + df_type + \"train_y_numpy.csv\").drop('Unnamed: 0', axis=1)\n",
        "\n",
        "  df20 = pd.read_csv(path_to_experiment + df_type + \"valid_acc.csv\").drop('Unnamed: 0', axis=1)\n",
        "  df21 = pd.read_csv(path_to_experiment + df_type + \"valid_loss.csv\").drop('Unnamed: 0', axis=1)\n",
        "  df22 = pd.read_csv(path_to_experiment + df_type + \"valid_pred.csv\").drop('Unnamed: 0', axis=1)\n",
        "  df23 = pd.read_csv(path_to_experiment + df_type + \"valid_time_per_epoch_list.csv\").drop('Unnamed: 0', axis=1)\n",
        "  df24 = pd.read_csv(path_to_experiment + df_type + \"valid_y.csv\").drop('Unnamed: 0', axis=1)\n",
        "  df25 = pd.read_csv(path_to_experiment + df_type + \"valid_y_numpy.csv\").drop('Unnamed: 0', axis=1)\n",
        "\n",
        "  df26 = pd.read_csv(path_to_experiment + df_type + \"true_mass_H_list_test\").drop('Unnamed: 0', axis=1)\n",
        "  df27 = pd.read_csv(path_to_experiment + df_type + \"true_mass_H_list_train\").drop('Unnamed: 0', axis=1)\n",
        "  df28 = pd.read_csv(path_to_experiment + df_type + \"true_mass_H_list_valid\").drop('Unnamed: 0', axis=1)\n",
        "  df29 = pd.read_csv(path_to_experiment + df_type + \"true_mass_O_list_test\").drop('Unnamed: 0', axis=1)\n",
        "  df30 = pd.read_csv(path_to_experiment + df_type + \"true_mass_O_list_train\").drop('Unnamed: 0', axis=1)\n",
        "  df31 = pd.read_csv(path_to_experiment + df_type + \"true_mass_O_list_valid\").drop('Unnamed: 0', axis=1)\n",
        "  df32 = pd.read_csv(path_to_experiment + df_type + \"true_mass_Z_list_test\").drop('Unnamed: 0', axis=1)\n",
        "  df33 = pd.read_csv(path_to_experiment + df_type + \"true_mass_Z_list_train\").drop('Unnamed: 0', axis=1)\n",
        "  df34 = pd.read_csv(path_to_experiment + df_type + \"true_mass_Z_list_valid\").drop('Unnamed: 0', axis=1)\n",
        "\n",
        "  # Fixed these two \n",
        "  df35 = pd.read_csv(path_to_experiment + df_type + \"train_pred_numpy.csv\").drop('Unnamed: 0', axis=1)\n",
        "  df36 = pd.read_csv(path_to_experiment + df_type + \"valid_pred_numpy.csv\").drop('Unnamed: 0', axis=1)\n",
        "  \n",
        "  return df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20, df21, df22, df23, df24, df25, df26, df27, df28, df29, df30, df31, df32, df33, df34, df35, df36"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTit8cyN1evx"
      },
      "source": [
        "# Performance metrics: JK 4 layer variable for KNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptEst14w1tJ6"
      },
      "source": [
        "# Path to experiment KNN\n",
        "path_to_experiment_variable_depth_4_jk_knn = '/content/drive/MyDrive/FCC_Experiments_2021/model_jk/variable/depth_4/aggr_lstm/knn/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cP0b3kx5Wh2"
      },
      "source": [
        "# Get dataframes for metrics\n",
        "df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20, df21, df22, df23, df24, df25, df26, df27, df28, df29, df30, df31, df32, df33, df34, df35, df36 =  get_model_outputs(path_to_experiment=path_to_experiment_variable_depth_4_jk_knn, df_type = 'df_knn_')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hEK7Slx12iP",
        "outputId": "44302d89-abf0-49e7-c4d1-1f7e5e3f5006"
      },
      "source": [
        "# Calculate metrics\n",
        "df_performance_metrics, df_average_performance_metrics, df_event_accuracy = get_performance_metrics(df_y=df13,  df_pred= df11, df_train_acc=df14,df_valid_acc=df20, df_test_acc=df10, path_to_experiment=path_to_experiment_variable_depth_4_jk_knn)\n",
        "df_accuracy_metrics, average_acc_train,average_acc_valid, average_loss_train, average_loss_valid, test_acc, final_test_acc, final_acc_train  = get_accuracy_metrics(df_train_acc= df14, df_valid_acc=df20, df_test_acc=df10, df_train_loss=df15, df_valid_loss=df21, path_to_experiment=path_to_experiment_variable_depth_4_jk_knn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      f1_score_macro  ...                                  matthews_corrcoef\n",
            "0  [1.0, 1.0, 1.0, 1.0, 0.896969696969697, 1.0, 1...  ...  [1.0, 1.0, 1.0, 1.0, 0.8115630216764502, 1.0, ...\n",
            "\n",
            "[1 rows x 4 columns]\n",
            "   f1_score_macro  f1_score_micro  f1_score_weighted  matthews_corrcoef\n",
            "0        0.984078        0.996791           0.997147           0.971254\n",
            "   event_accuracy_train  event_accuracy_valid  event_accuracy_test\n",
            "0              0.843549                 0.861             0.853961\n",
            "   final_acc_train  final_acc_valid  final_test_acc\n",
            "0         0.995367         0.995951        0.996791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61qZrse71lyz"
      },
      "source": [
        "# Performance metrics: JK 4 layer variable for Radius\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ag0J_sp2VBU"
      },
      "source": [
        "# Path to experiment for radius\n",
        "path_to_experiment_variable_depth_4_jk_radius = '/content/drive/MyDrive/FCC_Experiments_2021/model_jk/variable/depth_4/aggr_lstm/radius/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3DzU1Gk6za0"
      },
      "source": [
        "#  Get dataframes for metrics\n",
        "df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20, df21, df22, df23, df24, df25, df26, df27, df28, df29, df30, df31, df32, df33, df34, df35, df36 =  get_model_outputs(path_to_experiment=path_to_experiment_variable_depth_4_jk_radius, df_type = 'df_radius_')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVY0HRmQ7HIA",
        "outputId": "eb07216f-4972-4295-e51a-f46fe0a6d9e5"
      },
      "source": [
        "# Calculate metrics\n",
        "df_performance_metrics, df_average_performance_metrics, df_event_accuracy = get_performance_metrics(df_y=df13,  df_pred= df11, df_train_acc=df14,df_valid_acc=df20, df_test_acc=df10, path_to_experiment=path_to_experiment_variable_depth_4_jk_radius)\n",
        "df_accuracy_metrics, average_acc_train,average_acc_valid, average_loss_train, average_loss_valid, test_acc, final_test_acc, final_acc_train  = get_accuracy_metrics(df_train_acc= df14, df_valid_acc=df20, df_test_acc=df10, df_train_loss=df15, df_valid_loss=df21, path_to_experiment=path_to_experiment_variable_depth_4_jk_radius)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      f1_score_macro  ...                                  matthews_corrcoef\n",
            "0  [0.49264705882352944, 1.0, 0.4814814814814815,...  ...  [0.0, 1.0, 0.0, 0.6928203230275508, 1.0, 0.700...\n",
            "\n",
            "[1 rows x 4 columns]\n",
            "   f1_score_macro  f1_score_micro  f1_score_weighted  matthews_corrcoef\n",
            "0        0.695588          0.9724           0.960452           0.412813\n",
            "   event_accuracy_train  event_accuracy_valid  event_accuracy_test\n",
            "0              0.086755                 0.253                0.109\n",
            "   final_acc_train  final_acc_valid  final_test_acc\n",
            "0         0.964464         0.966554          0.9724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKukFxys3prp"
      },
      "source": [
        "# Performance metrics: JK 4 layer variable for Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCwnN9r-3OQz"
      },
      "source": [
        "# Path to experiment for label\n",
        "path_to_experiment_variable_depth_4_jk_label = '/content/drive/MyDrive/FCC_Experiments_2021/model_jk/variable/depth_4/aggr_lstm/label/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC_4EcTV7O6C"
      },
      "source": [
        "#  Get dataframes for metrics\n",
        "df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20, df21, df22, df23, df24, df25, df26, df27, df28, df29, df30, df31, df32, df33, df34, df35, df36 =  get_model_outputs(path_to_experiment=path_to_experiment_variable_depth_4_jk_label, df_type = 'df_label_')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjA3gOR67TDg",
        "outputId": "5007c0db-bc03-4d2e-8dd5-623c4c0a44ec"
      },
      "source": [
        "# Calculate metrics\n",
        "df_performance_metrics, df_average_performance_metrics, df_event_accuracy = get_performance_metrics(df_y=df13,  df_pred= df11, df_train_acc=df14,df_valid_acc=df20, df_test_acc=df10, path_to_experiment=path_to_experiment_variable_depth_4_jk_label)\n",
        "df_accuracy_metrics, average_acc_train,average_acc_valid, average_loss_train, average_loss_valid, test_acc, final_test_acc, final_acc_train  = get_accuracy_metrics(df_train_acc= df14, df_valid_acc=df20, df_test_acc=df10, df_train_loss=df15, df_valid_loss=df21, path_to_experiment=path_to_experiment_variable_depth_4_jk_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      f1_score_macro  ...                                  matthews_corrcoef\n",
            "0  [0.49264705882352944, 0.4878048780487805, 0.48...  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
            "\n",
            "[1 rows x 4 columns]\n",
            "   f1_score_macro  f1_score_micro  f1_score_weighted  matthews_corrcoef\n",
            "0          0.4895        0.958934           0.938869                0.0\n",
            "There are no events with 100% percent accuracy\n",
            "There are no events with 100% percent accuracy\n",
            "There are no events with 100% percent accuracy\n",
            "   event_accuracy_train  event_accuracy_valid  event_accuracy_test\n",
            "0                     0                     0                    0\n",
            "   final_acc_train  final_acc_valid  final_test_acc\n",
            "0         0.958222         0.958964        0.958934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfkx4993-5Gq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}