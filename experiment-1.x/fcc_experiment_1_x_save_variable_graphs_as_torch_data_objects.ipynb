{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fcc_experiment_1.x_save_variable_graphs_as_torch_data_objects.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMVq0gvGhR8Kd2qngMn2b9R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akanksha-ahuja/fcc-final-notebooks/blob/main/fcc_experiment_1_x_save_variable_graphs_as_torch_data_objects.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urofw919JkOb"
      },
      "source": [
        "#Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySBnIlgSJHkg"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "import timeit\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import distance\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-e1Tn2MJkyj",
        "outputId": "3f9cdc5f-4272-4b68-bb30-0c5651dacd44"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install graphlime\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from graphlime import GraphLIME\n",
        "from torch_geometric.utils import to_networkx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.0 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 222 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 376 kB 52.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.4 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting graphlime\n",
            "  Downloading graphlime-1.2.0.tar.gz (3.3 kB)\n",
            "Building wheels for collected packages: graphlime\n",
            "  Building wheel for graphlime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphlime: filename=graphlime-1.2.0-py3-none-any.whl size=2616 sha256=e5ae09cf5f667211e96e6675b39ac50e73ca97a5751af28d1130214c3756141b\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/29/94/9835c557e2def18b58369cda0032935a3263acfa9266aaeb5d\n",
            "Successfully built graphlime\n",
            "Installing collected packages: graphlime\n",
            "Successfully installed graphlime-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAqYK6R5JpHn"
      },
      "source": [
        "from torch_geometric.nn import GCNConv, TAGConv, SAGEConv, ChebConv\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.nn import GINConv\n",
        "from torch_geometric.nn import JumpingKnowledge, GCN2Conv\n",
        "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
        "from torch_geometric.nn import GNNExplainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMxe79BwJqoo"
      },
      "source": [
        "# Connect to G-drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrPEu9CKJr5i",
        "outputId": "4779b213-841f-44b8-c98b-0b966dbe25a6"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlhbxYbSJvPj"
      },
      "source": [
        "#Load and process df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3D6YrypJuNu"
      },
      "source": [
        "# Data Processing Functions\n",
        "def load_df(path_to_file):\n",
        "  df = pd.read_csv(path_to_file)\n",
        "  return df\n",
        "\n",
        "def set_constants(TOTAL_EVENTS, MAX_LENGTH_EVENT=150):\n",
        "  TOTAL_EVENTS = TOTAL_EVENTS\n",
        "  MAX_LENGTH_EVENT = MAX_LENGTH_EVENT\n",
        "  return TOTAL_EVENTS, MAX_LENGTH_EVENT\n",
        "\n",
        "def create_labels(df):\n",
        "  conditions = [(df['isHiggs'] == True),(df['isZ'] == True), (df['isOther'] == True) ]\n",
        "  # create a list of the values we want to assign for each condition\n",
        "  values = [0, 1, 2] \n",
        "\n",
        "  # create a new column and use np.select to assign values to it using our lists as arguments\n",
        "  df['label'] = np.select(conditions, values)\n",
        "  return df\n",
        "\n",
        "\n",
        "def normalise_x_features(df):\n",
        "  # Normalise the features in the dataset \n",
        "  df_id = df[['event_list']]\n",
        "  df_x = df[['pid', 'pos_r', 'pos_theta', 'pos_phi', 'pos_t', 'mom_p', 'mom_theta', 'mom_phi', 'mom_mass']]\n",
        "  df_y = df[['label']]\n",
        "\n",
        "  # Create a list of labels for the new dataframe\n",
        "  new_columns = ['event_list', 'pid', 'pos_r', 'pos_theta', 'pos_phi', 'pos_t', 'mom_p', 'mom_theta', 'mom_phi', 'mom_mass', 'label']\n",
        "\n",
        "  x = df_x.values # returns numpy \n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  df_x = pd.DataFrame(x_scaled)\n",
        "\n",
        "  # Concatenate normalised x features and un-normalised y labels and event ids\n",
        "  df_normalised_features = pd.concat([df_id, df_x, df_y], axis=1)\n",
        "  df_normalised_features.columns = new_columns # You need to mention the axis\n",
        "  return df_normalised_features\n",
        "\n",
        "def split_df_by_event(df_normalised_features, TOTAL_EVENTS):\n",
        "  # Dataframes split by event \n",
        "  df_event_list = []\n",
        "  for i in range(TOTAL_EVENTS):\n",
        "    df_event = df_normalised_features[df_normalised_features['event_list']==i]\n",
        "    df_event_list.append(df_event)\n",
        "\n",
        "  # A list of number of stable particles per event \n",
        "  length_of_each_event = [len(df_event_list[i]) for i in range(len(df_event_list))]\n",
        "  return df_event_list, length_of_each_event\n",
        "\n",
        "def create_source_target_for_COO(df_event_list):\n",
        "  # Add two columns of source, target over all dataframes in df_event_list to make it compatible with pygn Data Object.\n",
        "  df_event_source_target_list = []\n",
        "  for i in range(len(df_event_list)):\n",
        "    df_event_list[i]['source'] = None\n",
        "    df_event_list[i]['target'] = None\n",
        "    df_event_source_target_list.append(df_event_list[i])\n",
        "  return df_event_source_target_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOLt1wz8KBV0"
      },
      "source": [
        "#Generate Data.x and Data.y for pytorch geometric "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh2E0oPAJ-Xt"
      },
      "source": [
        "def generate_graph_data_x_y_for_already_normalised_features(df_event_processed_list_cleaned, TOTAL_EVENTS):\n",
        "  # Generating data.x and data.y for pytorch geomteric \n",
        "  graph_data_x_list = []\n",
        "  graph_data_y_list = []\n",
        "  for i in range(TOTAL_EVENTS):\n",
        "    df_graph = df_event_processed_list_cleaned[i]\n",
        "    # Extract node features and labels from cleaned processed fixed size event list and convert to numpy \n",
        "    data_x = df_graph[['pid', 'pos_r', 'pos_theta', 'pos_phi', 'pos_t', 'mom_p', 'mom_theta', 'mom_phi', 'mom_mass']].to_numpy()\n",
        "    data_y = df_graph[['label']].to_numpy()\n",
        "\n",
        "    # Convert numpy objects into tensors for data loaders \n",
        "    graph_data_x_list.append(torch.Tensor(data_x))\n",
        "    graph_data_y_list.append(torch.Tensor(data_y))\n",
        "  return graph_data_x_list, graph_data_y_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHpQeKfxKHUP"
      },
      "source": [
        "# Create graph nodes and labels "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy0wcg73KFjj"
      },
      "source": [
        "def create_graph_nodes_and_labels_for_variable_graphs(path_to_file, TOTAL_EVENTS):\n",
        "  df = load_df(path_to_file) # You can specify path to file here \n",
        "  TOTAL_EVENTS, _ = set_constants(TOTAL_EVENTS) # you can pass the constants here \n",
        "  df = create_labels(df) \n",
        "  df_normalised_features = normalise_x_features(df) # Don't call this if you are normalising features when creating graph dataset for FIXED GRAPHS  \n",
        "  df_event_list, length_of_each_event = split_df_by_event(df_normalised_features, TOTAL_EVENTS)\n",
        "  df_event_source_target_list = create_source_target_for_COO(df_event_list) \n",
        "  df_event_processed_list_cleaned = df_event_source_target_list\n",
        "  graph_data_x_list, graph_data_y_list = generate_graph_data_x_y_for_already_normalised_features(df_event_processed_list_cleaned, TOTAL_EVENTS)\n",
        "  return df, df_normalised_features, df_event_list, length_of_each_event, df_event_processed_list_cleaned, graph_data_x_list, graph_data_y_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SKRYrNUKNNL"
      },
      "source": [
        "# Label "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVZT59IcKN5z"
      },
      "source": [
        "def generate_particle_lists_for_label_connections(df_event_list):\n",
        "  # Getting all the lists for each event in the dataset - define h_list, z_list, o_list\n",
        "  h_list = [] # all higgs nodes are connected Ω\n",
        "  z_list = []  # all z nodes are connected \n",
        "  o_list = [] # all otehr nodes are connected \n",
        "\n",
        "  for i in range(len(df_event_list)):\n",
        "    df_event = df_event_list[i]\n",
        "    h = df_event[df_event['label']==0]\n",
        "    h.reset_index(drop=True)\n",
        "    z = df_event[df_event['label']==1]\n",
        "    z.reset_index(drop=True)\n",
        "    o = df_event[df_event['label']==2] \n",
        "    o.reset_index(drop=True)\n",
        "    h_list.append(h)\n",
        "    z_list.append(z)\n",
        "    o_list.append(o)\n",
        "\n",
        "  return h_list, z_list, o_list\n",
        "\n",
        "def generate_X_label_list(h_list, z_list, o_list, df_event_list, TOTAL_EVENTS):\n",
        "  Z_BOSON = int(1)\n",
        "  H_BOSON = int(0)\n",
        "  NO_BOSON = int(2)\n",
        "  # NO_PARTICLE = int(3)\n",
        "  X_list = []\n",
        "  # for each event event_id \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    X = df_event_list[event_id]\n",
        "    source_list, target_list  = [], []\n",
        "    # for each node_id \n",
        "    for node_id in range(len(X)):\n",
        "      if X.iloc[node_id].label == Z_BOSON:\n",
        "        source = [node_id for x in range(len(z_list[event_id]))]\n",
        "        target = [x for x in range(len(z_list[event_id]))]\n",
        "        source_list.append(source)\n",
        "        target_list.append(target)\n",
        "      elif X.iloc[node_id].label == H_BOSON:\n",
        "        source = [node_id for x in range(len(h_list[event_id]))]\n",
        "        target = [x for x in range(len(h_list[event_id]))]\n",
        "        source_list.append(source)\n",
        "        target_list.append(target)\n",
        "      elif X.iloc[node_id].label == NO_BOSON:\n",
        "        source = [node_id for x in range(len(o_list[event_id]))]\n",
        "        target = [x for x in range(len(o_list[event_id]))]\n",
        "        source_list.append(source)\n",
        "        target_list.append(target)\n",
        "      # Add all values as 2 columns for eache event   \n",
        "    X['source'] = source_list\n",
        "    X['target'] = target_list\n",
        "    X_list.append(X)\n",
        "\n",
        "  return X_list\n",
        "\n",
        "def convert_coo_format_for_label_events(df):\n",
        "    source_list = list(itertools.chain.from_iterable(df['source'].to_numpy())) \n",
        "    target_list = list(itertools.chain.from_iterable(df['target'].to_numpy()))\n",
        "    edge_index= torch.tensor([source_list, target_list], dtype=torch.long)\n",
        "    return edge_index\n",
        "    \n",
        "def create_COO_format_data_label_list(X_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS):\n",
        "  # Data Represented as edges with same labels \n",
        "  # A list of graph data items to be passed on to the data loader \n",
        "  data_label_list = []\n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    data_item = Data(x = graph_data_x_list[event_id], \n",
        "                     y = graph_data_y_list[event_id], \n",
        "                    edge_index = convert_coo_format_for_label_events(X_list[event_id]))\n",
        "    data_label_list.append(data_item)\n",
        "  return data_label_list\n",
        "\n",
        "def create_data_label_list(df_event_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS):\n",
        "  # Create Data Label List \n",
        "  h_list, z_list, o_list  = generate_particle_lists_for_label_connections(df_event_list)\n",
        "  X_list = generate_X_label_list(h_list, z_list, o_list, df_event_list, TOTAL_EVENTS)\n",
        "  data_label_list = create_COO_format_data_label_list(X_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS)\n",
        "  return data_label_list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaa6McDjKRPz"
      },
      "source": [
        "#Radius"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5vQBcrsKR1n"
      },
      "source": [
        "def get_features_extraction_list(df_event_list, TOTAL_EVENTS):\n",
        "  feature_extraction_list = [] \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    event_features = df_event_list[event_id][[\"pid\",\t\"pos_r\",\t\"pos_theta\",\t\"pos_phi\",\t\"pos_t\"\t,\"mom_p\",\t\"mom_theta\",\t\"mom_phi\",\t\"mom_mass\"]]\n",
        "    feature_extraction_list.append(event_features)\n",
        "  return feature_extraction_list\n",
        "\n",
        "def get_PCA_transformed_features(feature_extraction_list, TOTAL_EVENTS):\n",
        "  X_pca_list = []   \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    pca = PCA() \n",
        "    X_pca = pca.fit_transform(feature_extraction_list[event_id])\n",
        "    X_pca_list.append(X_pca)\n",
        "  return X_pca_list\n",
        "\n",
        "def get_2_D_coordinates(X_pca_list, length_of_each_event, TOTAL_EVENTS):\n",
        "  # Find all 2-d coordinates \n",
        "  point_event_list = [] # stores all points for each event in a list of tuple points\n",
        "  index_event_list = [] # stores all indices for each event in a list of tuple indices\n",
        "  principal_components_list = [5, 6] # after data exploration, these two were chosen \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    length = length_of_each_event[event_id]\n",
        "    points = []\n",
        "    index_list = []\n",
        "    # print(length)\n",
        "    for node_id_source in range(length):\n",
        "      for node_id_target in range(length):\n",
        "        pt = (X_pca_list[event_id][node_id_source, principal_components_list[0]],\n",
        "              X_pca_list[event_id][node_id_target, principal_components_list[1]])\n",
        "        index_list.append((node_id_source, node_id_target))\n",
        "        points.append(pt)\n",
        "    point_event_list.append(points)\n",
        "    index_event_list.append(index_list)\n",
        "  return point_event_list, index_event_list\n",
        "\n",
        "def calculate_euclidean_distance(point_event_list, length_of_each_event, TOTAL_EVENTS):\n",
        "  # Calculate euclidean distance between each consecutive pair \n",
        "  distance_event_list = []\n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    event_length = length_of_each_event[event_id]\n",
        "    distance_list = []\n",
        "    for node_id_source in range(event_length):\n",
        "      for node_id_target in range(event_length):\n",
        "        # print(k, length_of_each_event[k], i, j)\n",
        "        xpt = point_event_list[event_id][node_id_source][0]\n",
        "        ypt = point_event_list[event_id][node_id_target][1]\n",
        "        dist = distance.euclidean(xpt,ypt)\n",
        "        distance_list.append(dist)\n",
        "    distance_event_list.append(distance_list)\n",
        "  return distance_event_list\n",
        "\n",
        "def calculate_node_distances_by_event(distance_event_list, length_of_each_event, TOTAL_EVENTS):\n",
        "  # Calculating the each node's distances for event_length for each event for 10,000 events\n",
        "  distance_each_particle_event_list = []\n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    distance_each_particle_list = np.array_split(distance_event_list[event_id], length_of_each_event[event_id]) \n",
        "    distance_each_particle_event_list.append(distance_each_particle_list)\n",
        "  return distance_each_particle_event_list\n",
        "\n",
        "def calculate_edges_by_radius(distance_each_particle_event_list, length_of_each_event, TOTAL_EVENTS, radius = 0.2):\n",
        "  # Fixing radius at random or threshold to be 0.2, therefore all node_ids with distances less than this threshold, is added to the neighbour_list \n",
        "  neighbour_event_list = []\n",
        "  target_event_list = []\n",
        "  source_event_list = [] \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    event_length = length_of_each_event[event_id]\n",
        "    neighbour_list = []\n",
        "    source_list = []\n",
        "    target_list = []\n",
        "    for node_id_source in range(event_length):\n",
        "      for node_id_target in range(event_length):\n",
        "        if distance_each_particle_event_list[event_id][node_id_source][node_id_target] <= radius:\n",
        "            source_list.append(node_id_source)\n",
        "            target_list.append(node_id_target)\n",
        "            neighbour_list.append((node_id_source, node_id_target))\n",
        "    neighbour_event_list.append(neighbour_list)\n",
        "    target_event_list.append(target_list)\n",
        "    source_event_list.append(source_list)\n",
        "  return neighbour_event_list, target_event_list, source_event_list\n",
        "\n",
        "def convert_coo_format_for_radius_events(source_event_list,target_event_list, event_id):\n",
        "    edge_index= torch.tensor([source_event_list[event_id], target_event_list[event_id]], dtype=torch.long)\n",
        "    return edge_index\n",
        "\n",
        "def create_COO_format_data_radius_list(source_event_list,target_event_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS):\n",
        "  # Data Represented as edges within Radius = 0.2 \n",
        "  # A list of graph data items to be passed on to the data loader \n",
        "  data_radius_list = []\n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    data_item = Data(x = graph_data_x_list[event_id], \n",
        "                    y = graph_data_y_list[event_id], \n",
        "                    edge_index = convert_coo_format_for_radius_events(source_event_list,target_event_list, event_id))\n",
        "    data_radius_list.append(data_item)\n",
        "  return data_radius_list\n",
        "\n",
        "def create_data_radius_list(df_event_list, length_of_each_event, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, radius = 0.2):\n",
        "  # Create Data Radius List \n",
        "  feature_extraction_list = get_features_extraction_list(df_event_list, TOTAL_EVENTS)\n",
        "  X_pca_list = get_PCA_transformed_features(feature_extraction_list, TOTAL_EVENTS) \n",
        "  point_event_list, index_event_list = get_2_D_coordinates(X_pca_list, length_of_each_event, TOTAL_EVENTS) \n",
        "  distance_event_list = calculate_euclidean_distance(point_event_list, length_of_each_event, TOTAL_EVENTS)\n",
        "  distance_each_particle_event_list = calculate_node_distances_by_event(distance_event_list, length_of_each_event,  TOTAL_EVENTS)\n",
        "  neighbour_event_list, target_event_list, source_event_list = calculate_edges_by_radius(distance_each_particle_event_list, length_of_each_event, TOTAL_EVENTS, radius = 0.2)\n",
        "  data_radius_list = create_COO_format_data_radius_list(source_event_list,target_event_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS)\n",
        "  return data_radius_list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2POD5lyKVp0"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlFJHkAzKWOc"
      },
      "source": [
        "def define_knn(num_neighbours=8):\n",
        "  knn = NearestNeighbors(n_neighbors=num_neighbours, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\n",
        "  return knn\n",
        "\n",
        "def generate_X_list_knn(knn, df_event_list, TOTAL_EVENTS, num_neighbours=8):\n",
        "  X_list_knn = [] \n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    X = df_event_list[event_id]\n",
        "    X = X.drop(columns=['source', 'target'])\n",
        "    knn.fit(X)\n",
        "    neighbour = knn.kneighbors(X, n_neighbors=num_neighbours, return_distance=False)\n",
        "    target = neighbour\n",
        "    source = np.zeros((neighbour.shape))\n",
        "    X['source'] = None\n",
        "    X['target'] = None\n",
        "    for i in range(len(X)):\n",
        "      X['source'].iloc[i] = np.ones((neighbour.shape)) * i\n",
        "      X['target'].iloc[i] = target[i]\n",
        "    X_list_knn.append(X)\n",
        "  return X_list_knn\n",
        "\n",
        "def convert_COO_for_knn_events(df, num_neighbours=8):\n",
        "  source_list = [] \n",
        "  for i in range(len(df)):\n",
        "    for _ in range(num_neighbours):\n",
        "      source_list.append(i)\n",
        "  target_list = list(itertools.chain.from_iterable(df['target'].to_numpy()))\n",
        "  edge_index= torch.tensor([source_list, target_list], dtype=torch.long)\n",
        "  return edge_index\n",
        "\n",
        "def create_COO_format_data_knn_list(X_list_knn, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, num_neighbours=8):\n",
        "  # Data Represented as edges with K-nearest neighbours as 8 \n",
        "  # A list of graph data items to be passed on to the data loader \n",
        "  data_knn_list = []\n",
        "  for event_id in range(TOTAL_EVENTS):\n",
        "    data_item = Data(x = graph_data_x_list[event_id], \n",
        "                    y = graph_data_y_list[event_id], \n",
        "                    edge_index = convert_COO_for_knn_events(X_list_knn[event_id], num_neighbours=8))\n",
        "    data_knn_list.append(data_item)\n",
        "  return data_knn_list\n",
        "\n",
        "def create_data_knn_list(df_event_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, num_neighbours=8):\n",
        "  # Create data knn list \n",
        "  knn = define_knn(num_neighbours=8)\n",
        "  X_list_knn = generate_X_list_knn(knn, df_event_list, TOTAL_EVENTS, num_neighbours=8)\n",
        "  data_knn_list = create_COO_format_data_knn_list(X_list_knn, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, num_neighbours=8)\n",
        "  return data_knn_list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSoYvYnKoIJ"
      },
      "source": [
        "#Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFZXPDCDKcjC"
      },
      "source": [
        "def get_data_lists_for_variable_size_graphs(path_to_file, TOTAL_EVENTS):\n",
        "  # VARIABLE SIZE GRAPHS\n",
        "  df, df_normalised_features, df_event_list, length_of_each_event, df_event_processed_list_cleaned, graph_data_x_list, graph_data_y_list = create_graph_nodes_and_labels_for_variable_graphs(path_to_file, TOTAL_EVENTS)\n",
        "  data_label_list = create_data_label_list(df_event_list, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS)\n",
        "  data_radius_list = create_data_radius_list(df_event_list, length_of_each_event, graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, radius = 0.2)\n",
        "  data_knn_list  = create_data_knn_list(df_event_list,graph_data_x_list, graph_data_y_list, TOTAL_EVENTS, num_neighbours=8)\n",
        "  return data_radius_list, data_knn_list, data_label_list, df_event_list, length_of_each_event, df_event_processed_list_cleaned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_01RjVWKamq"
      },
      "source": [
        "#Creating, Saving and Loading Graphs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKoi7Xf-nGUx"
      },
      "source": [
        "def save_data_list_as_torch_data_object(data_list,  path_to_save_data_object, TOTAL_EVENTS):\n",
        "  # TORCH SAVING\n",
        "  for x in range(TOTAL_EVENTS):\n",
        "    torch.save(data_list[x], path_to_save_data_object + 'event_' + str(x) + '.pickle')\n",
        "  return \"Saved \" + str(TOTAL_EVENTS) +\" data objects\"\n",
        "\n",
        "def load_data_list_as_torch_object( path_to_save_data_object, TOTAL_EVENTS):\n",
        "  data_list_loaded_list = []\n",
        "  # TORCH LOADING \n",
        "  for x in range(TOTAL_EVENTS):\n",
        "    data_list_x = Data(torch.load( path_to_save_data_object +'event_'+ str(x) + '.pickle'))\n",
        "    data_list_loaded_list.append(data_list_x)\n",
        "  return data_list_loaded_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMadjwJJMhiM"
      },
      "source": [
        "# MENTION NUMBER OF EVENTS and path to file\n",
        "TOTAL_EVENTS = 10\n",
        "path_to_file=\"output_11_07_2021.csv\"\n",
        "data_radius_list_variable, data_knn_list_variable, data_label_list_variable, df_event_list_variable, length_of_each_event_variable, df_event_processed_list_cleaned_variable = get_data_lists_for_variable_size_graphs(path_to_file, TOTAL_EVENTS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbWSzgoDpblg"
      },
      "source": [
        "path_to_save_data_object_radius = '/content/drive/MyDrive/FCC_Experiments_2021/dataset/pytorch_geometric_data_lists/variable/data_radius_list/'\n",
        "path_to_save_data_object_knn = '/content/drive/MyDrive/FCC_Experiments_2021/dataset/pytorch_geometric_data_lists/variable/data_knn_list/'\n",
        "path_to_save_data_object_label = '/content/drive/MyDrive/FCC_Experiments_2021/dataset/pytorch_geometric_data_lists/variable/data_label_list/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GereZ7SiuH-m",
        "outputId": "de808b08-b3a4-4ebe-9b75-998017fd6630"
      },
      "source": [
        "save_data_list_as_torch_data_object(data_radius_list_variable, path_to_save_data_object= path_to_save_data_object_radius +'data_radius_list_variable_', TOTAL_EVENTS=10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Saved 10 data objects'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Qa9zr_utuIGw",
        "outputId": "278f672a-3e70-4ea7-be47-852e8d1e8d1e"
      },
      "source": [
        "save_data_list_as_torch_data_object(data_radius_list_variable, path_to_save_data_object= path_to_save_data_object_radius +'data_radius_list_variable_', TOTAL_EVENTS=10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Saved 10 data objects'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sOZa0q8IuKdI",
        "outputId": "3bb56b07-f6f6-4efc-9644-626d4872b4de"
      },
      "source": [
        "save_data_list_as_torch_data_object(data_label_list_variable , path_to_save_data_object=path_to_save_data_object_label +'data_label_list_variable_', TOTAL_EVENTS=10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Saved 10 data objects'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-tT-XQVoiDH"
      },
      "source": [
        "# save_data_list_as_torch_data_object(data_knn_list_variable,  path_to_save_data_object= path_to_save_data_object_knn + 'data_knn_list_variable_', TOTAL_EVENTS=10)\n",
        "# save_data_list_as_torch_data_object(data_radius_list_variable, path_to_save_data_object= path_to_save_data_object_radius +'data_radius_list_variable_', TOTAL_EVENTS=10)\n",
        "# save_data_list_as_torch_data_object(data_label_list_variable , path_to_save_data_object=path_to_save_data_object_label +'data_label_list_variable_', TOTAL_EVENTS=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4LZTID9h1fU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
